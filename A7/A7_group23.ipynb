{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2481f2a-2a27-4e04-8b63-b3fdb983ad1c",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a719f1-96f9-46d1-a6d0-86bc99cebeec",
   "metadata": {},
   "source": [
    "## Part 1: Canned procedures for p-value adjustment\n",
    "In this part we are going to explore which variables are relevant for first run U.S. box office ($) sales, for a set of 62 movies.\n",
    "We have 12 explanatory variables:\n",
    "\n",
    "* MPRating = MPAA Rating code, 1=G, 2=PG, 3=PG13, 4=R,\n",
    "* Budget = Production budget ($Mil),\n",
    "* Starpowr = Index of star poser,\n",
    "* Sequel = 1 if movie is a sequel, 0 if not,\n",
    "* Action = 1 if action film, 0 if not,\n",
    "* Comedy = 1 if comedy film, 0 if not,\n",
    "* Animated = 1 if animated film, 0 if not,\n",
    "* Horror = 1 if horror film, 0 if not,\n",
    "* Addict = Trailer views at traileraddict.com,\n",
    "* Cmngsoon = Message board comments at comingsoon.net,\n",
    "* Fandango = Attention at fandango.com (see Example 4.12),\n",
    "* Cntwait3 = Percentage of Fandango votes that can't wait to see\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de5df485-c2eb-4bfe-8006-3dbff4009c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#os.chdir(\"change working directory if needed\")\n",
    "\n",
    "movie_data = pd.read_csv(\"movie_buzz.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143f0e5-378b-47e5-8bf4-d9938cdd7377",
   "metadata": {},
   "source": [
    "### Task 1a)\n",
    "\n",
    "We want to train a linear regression model with the *logarithm* of box office sales as output and all other variables in `movie_data` (plus a constant) as inputs. We are not going to hold out any validation data. \n",
    "\n",
    "First, extract the output and input variables into $n \\times 1$ and $n \\times p$ NumPy arrays `y_1a` and `X_1a`, respectively. Second, use the `OLS()` function in `statsmodels` to specify the regression model and to learn its coefficients. Save the specified model as `lm_fit_1a`. Third, save the p-values of tests for the individual significance of the regression coefficients as `pvalues_1a`. \n",
    "\n",
    "*Hint:* Information about where the p-values of a learned linear regression can be found is provided by the [documentation](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.RegressionResults.html#statsmodels.regression.linear_model.RegressionResults) of the `RegressionResults()` class in `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1ad2582-fde1-4f7a-80f7-6e64713ecd2e",
   "metadata": {
    "tags": [
     "code_chunk_01"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.616\n",
      "Model:                            OLS   Adj. R-squared:                  0.522\n",
      "Method:                 Least Squares   F-statistic:                     6.554\n",
      "Date:                Wed, 29 Oct 2025   Prob (F-statistic):           8.52e-07\n",
      "Time:                        21:21:02   Log-Likelihood:                -54.149\n",
      "No. Observations:                  62   AIC:                             134.3\n",
      "Df Residuals:                      49   BIC:                             162.0\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         15.2537      0.598     25.512      0.000      14.052      16.455\n",
      "x1            -0.2122      0.133     -1.597      0.117      -0.479       0.055\n",
      "x2             0.0052      0.003      1.675      0.100      -0.001       0.011\n",
      "x3            -0.0047      0.013     -0.370      0.713      -0.030       0.021\n",
      "x4             0.3939      0.318      1.237      0.222      -0.246       1.034\n",
      "x5            -0.7491      0.281     -2.669      0.010      -1.313      -0.185\n",
      "x6            -0.0016      0.247     -0.007      0.995      -0.497       0.494\n",
      "x7            -0.8212      0.387     -2.121      0.039      -1.599      -0.043\n",
      "x8             0.4377      0.357      1.228      0.225      -0.279       1.154\n",
      "x9          2.216e-05   1.59e-05      1.390      0.171   -9.88e-06    5.42e-05\n",
      "x10           -0.0001      0.001     -0.125      0.901      -0.002       0.002\n",
      "x11            0.0002      0.000      0.725      0.472      -0.000       0.001\n",
      "x12            3.2915      0.803      4.097      0.000       1.677       4.906\n",
      "==============================================================================\n",
      "Omnibus:                        0.796   Durbin-Watson:                   2.076\n",
      "Prob(Omnibus):                  0.672   Jarque-Bera (JB):                0.253\n",
      "Skew:                           0.011   Prob(JB):                        0.881\n",
      "Kurtosis:                       3.312   Cond. No.                     1.01e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.01e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_01\n",
    "# 1.\n",
    "y_1a = np.log(movie_data[\"BOX\"].values).reshape(-1, 1)\n",
    "X_1a = movie_data.drop(columns=[\"BOX\"]).values\n",
    "X_1a = sm.add_constant(X_1a)\n",
    "\n",
    "# 2.\n",
    "lm_fit_1a = sm.OLS(y_1a, X_1a).fit()\n",
    "print(lm_fit_1a.summary())\n",
    "\n",
    "# 3.\n",
    "pvalues_1a = lm_fit_1a.pvalues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db13a6-b375-41d8-bf77-df0f7569bcf3",
   "metadata": {},
   "source": [
    "### Task 1b)\n",
    "\n",
    "The `multitest()` function in the `stats.multipletest` module of `statsmodels` calculates p-value adjustments that control either the familywise error rate (FWER) or the false discovery rate (FDR). A list of the supported methods is given in the [statsmodels documentation](https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html).\n",
    "\n",
    "Use this function to correct the p-values `p_val_1a` using Holm's procedure and save them as p_holm_1b. Then, store the names of all input variables whose corrected p-value is below the significance level `alpha_1b` in an object `names_selected_1b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d21fe31-370f-46e2-b705-29b4f2849a4d",
   "metadata": {
    "tags": [
     "code_chunk_02"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['const' 'CNTWAIT3 ']\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_02\n",
    "import statsmodels.stats.multitest as smm\n",
    "\n",
    "alpha_1b = 0.05\n",
    "\n",
    "# 1.\n",
    "p_holm_1b = smm.multipletests(pvalues_1a, alpha=alpha_1b, method='holm')[1]\n",
    "\n",
    "# 2.\n",
    "names = np.array(['const'] + list(movie_data.drop(columns=['BOX']).columns))\n",
    "names_selected_1b = names[p_holm_1b < alpha_1b]\n",
    "print(names_selected_1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f939b8a-c63a-4aa7-9212-0c33e345abf6",
   "metadata": {},
   "source": [
    "## Part 2: The FWER in a simulation study\n",
    "\n",
    "In this part, we will use simulations to convince ourselves that standard p-values are problematic if we conduct a larger number of tests and that simple p-value adjustments can address our multiple testing problem.\n",
    "\n",
    "Below, I have written a short function `X_fun_precompute` that calculates particular functions of some provided matrix `XX`. These are returned as a dictionary. We will need this function output in the tasks below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d66bfeb7-a01a-4ba1-aad9-01d91bd0dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_fun_precompute(XX):\n",
    "    p1        = XX.shape[1]\n",
    "    XtX       = XX.T @ XX\n",
    "    XtX_inv   = np.linalg.solve(XtX, np.eye(p1))\n",
    "    XtX_inv_Xt = XtX_inv @ XX.T  \n",
    "    results = {'X': XX,\n",
    "               'XtX': XtX,\n",
    "               'XtX_inv': XtX_inv,\n",
    "               'XtX_inv_Xt': XtX_inv_Xt}\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42bdc94-8f9e-499f-9596-26b1cc56ba0c",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "We start with the individual building blocks of our simulation study. First, we obtained learned coefficients, residuals and variance estimates from a linear regression model. Do the following:\n",
    "\n",
    "1. Call the function `X_fun_precompute` with `X_1a` as its only argument and save the function output as `X_funs_2a`.\n",
    "2. Manually compute the learned coefficients in a regression model with `y_1a` as output and `X_1a` as inputs. Save it as `beta_hat_2a`. Use the pre-computed functions of `X_1a` inside `X_funs_2a` instead of constructing them again.  \n",
    "3. Obtain the marginal variance \n",
    "$$ \\widehat{Var}[\\hat{\\beta} | X, y]= \\hat{\\sigma}^{2}(X^TX)^{-1} $$\n",
    "where $\\hat{\\sigma}^2$ is the estimated variance of the residuals. Save it as `VX_2a`. Again, use objects inside `X_funs_2a` to refer to `X_1a` and any function that involves `X_1a` alone. \n",
    "\n",
    "*Hint:* A good way to see that you have done things correctly is to compare your result with the summary of your regression results in Task 1a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759ccb6-fbd1-42d8-beff-8bbd349075e5",
   "metadata": {
    "tags": [
     "code_chunk_03"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_03\n",
    "# 1.\n",
    "X_funs_2a = ??\n",
    "\n",
    "# 2.\n",
    "??\n",
    "\n",
    "# 3.\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ba36b-2764-4b6e-a946-00eb9df8ed81",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "\n",
    "As a next step, do the following:\n",
    "\n",
    "1. Obtain $t-$ statistics for $\\beta_i$ under $H_0:\\beta_i=0$, namely\n",
    "$$\n",
    "t_i = \\left|\\frac{\\hat{\\beta}_i}{\\widehat{sd[\\hat{\\beta}_i | X, y]}} \\right|,\n",
    "$$\n",
    "and save them as `t_2b`. Use existing objects from Task 2a to arrive there.\n",
    "2. Compute the corresponding $p-$values of two-sided tests and save them as `pvalues_2b`. Here, keep in mind that $\\frac{\\hat{\\beta}_i}{\\widehat{sd[\\hat{\\beta}_i | X, y]}} $ follows a Student-t distribution with $n-p$ degrees of freedom. The cumulative density function of this distribution is given by the `t.cdf()` function in the stats-module of `scipy`. See its documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html)\n",
    "\n",
    "*Hint*: Use the result\n",
    "$$\n",
    "P_{0}(|T| \\geq t) = P_{0}(T \\leq -t)  + P_{0}(T \\geq t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ebd29-01b1-4101-bdda-8476f78db551",
   "metadata": {
    "tags": [
     "code_chunk_04"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_04\n",
    "from scipy.stats import t\n",
    "\n",
    "# 1.\n",
    "t_2b       = ??\n",
    "\n",
    "# 2.\n",
    "pvalues_2b = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e77c0-96da-4351-8bed-135a149f11bb",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "Now write a function `calculate_p_2c`.  Its arguments are a $n times 1$ Numpy array of output values `y0` as well as a dictionary `X0_funs`  with the same structure as `X_funs_2a`. `calculate_p_2c` should conduct the same steps as in Tasks 2a-b and should return the p-values of a regression of `y0` on the input matrix from which `X0_funs` has been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19df31f-e2ee-45ce-9450-c3b6436f6a58",
   "metadata": {
    "tags": [
     "code_chunk_05"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_05\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f6f59-852d-4605-bfe5-a2b9a7545cc6",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "Now we can set op our simulation study. Write a loop that repeats the following steps 1000 times:\n",
    "\n",
    "1. Simulate an output `y_fake` unrelated to the existing inputs `X_1a`. `y_fake` should be drawn from a normal distribution with mean and variance coming from the sample mean and variance of the observed $y$ in your dataset.\n",
    "\n",
    "2. Use your function `calculate_p_2c` to compute $p-$values in a linear regression of `y_fake` on the observed inputs `X_1a`. Use the existing dictionary `X_funs_2a` as second function argument.\n",
    "3. Adjust the p-values returned by `calculate_p_2c` in two different ways: i) Bonferroni adjustment, ii) Holm's method.\n",
    "4. Store the number of variables in the learned model (excluding the intercept) whose coefficient is found to be statistically significant at a significance level $\\alpha=0.05$. Do this for i) p-value without adjustment, ii) Bonferroni-adjusted p-values, iii) Holm-adjusted p-values.\n",
    "\n",
    "For step 3., create empty arrays, `n_selected_naive`, `n_selected_Holm` and `n_selected_Bonferroni` before you write the for loop. Inside the loop, store the number of statistically significant inputs in your *j*-th iteration in element *j* of these objects.\n",
    "\n",
    "When the loop has finished, calculate the FWER using results from the 100 iterations of your loop. Save the FWER for your three sets of p-value based selection of significant predictors as `FWER_Naive`, `FWER_Holm` and `FWER_BF`.\n",
    "\n",
    "Does either of the three methods successfully control the FWER? Save your answer in the string variable `FWER_control_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7dbcb-f8dc-4c91-b6ba-a6577915a1e0",
   "metadata": {
    "tags": [
     "code_chunk_06"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_06\n",
    "??\n",
    "\n",
    "FWER_control_2d = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150da53-2d59-4f4a-8f64-4e5a14cf18a5",
   "metadata": {},
   "source": [
    "## Part 3: Test power with p-value adjustment\n",
    "\n",
    "Now that we have created an efficient algorithm for computing the p-values, we are going to explore what happens when one adds several signals to the simulated data.\n",
    "However, we first rescale all input variables in our data so that they have a variance of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add7dd46-7627-483e-afda-e9de1aa59ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_3a = StandardScaler(with_mean=False)\n",
    "X_3a      = scaler_3a.fit_transform(X_1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a80fb7-3434-4851-993b-cd7c62f21fc6",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "Next, we simulate artificial outcomes. However, in contrast to task 2d we are *not* creating new $y$ that are completely unrelated to the observed predictors in our box office sales data. More specifically, we simulate $y$ from the linear model\n",
    "$$\n",
    "\\mathbf{y}= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n",
    "$$\n",
    "where $X$ contains the scaled predictors of Task 3a. For the vector of slope coefficients $\\beta$, we let $\\beta_{2:5}=log(2:5)$ whereas all other elements of this vector are 0. The model errors $\\epsilon$ are drawn independently from a standard normal distribution. Construct a new object `y_sim_3a` that contains simulated outcomes from the model described here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cbc21-441d-454c-8949-efcfc97305e5",
   "metadata": {
    "tags": [
     "code_chunk_07"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_07\n",
    "signal_index_3a = [1, 2, 3, 4]\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec76254-150b-48ee-a7d0-c5ae1b268c55",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "In this task were are getting adjusted p-values again:\n",
    "\n",
    "1. Obtain a list object `x_funs_3b` containing functions of `X_3a` using `X_fun_precompute()`.\n",
    "2. Use your`calculate_p_2c()` function to get p-values for significance tests in a regression with `y_sim_3a` as output and `X_3a` as inputs. Save your result as `pvalues_3b`.\n",
    "2. Get adjusted p-values from Bonferroni and Benjamini & Hochberg corrections by using `p.adjust()`. Save these p-values as `p_BF_3b` and `p_hochberg_3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484942e-9828-4d4b-8328-279087245812",
   "metadata": {
    "tags": [
     "code_chunk_08"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_08\n",
    "# 1.\n",
    "x_funs_3b  = ??\n",
    "\n",
    "# 2.\n",
    "pvalues_3b = ??\n",
    "\n",
    "# 3.\n",
    "p_BF_3b       = ??\n",
    "p_hochberg_3b = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f703f-0c4a-4bbf-bf78-1a672c549688",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "Next, we calculate the false discovery proportion of Bonferroni and Benjamini-Hochberg methods.\n",
    "\n",
    "1. Create  binary vectors `selected_BF_3c` and `selected_hochberg_3c` whose elements indicate which input variables are significant at a (familywise) significance level of $\\alpha=0.15$. \n",
    "2. Create  binary vectors `selected_true_BF_3c` and `selected_true_hochberg_3c` whose elements indicate if the coefficient on a particular input variable is significant *and* has a nonzero true value in the setup that you used to generate `y_sim_3a`. \n",
    "3. Use the objects from steps 1 and 2 to calculate the $fdp$ for Bonferroni and Benjamini & Hochberg corrections. Save them as `fdp_BF_3c` and `fdp_hochberg_3c` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6ae446-b379-40dd-b283-0db008207946",
   "metadata": {
    "tags": [
     "code_chunk_09"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_09\n",
    "alpha_3c = 0.15\n",
    "\n",
    "# 1.\n",
    "selected_BF_3c       = ??\n",
    "selected_hochberg_3c = ??\n",
    "\n",
    "# 2.\n",
    "selected_true_BF_3c       = ??\n",
    "selected_true_hochberg_3c = ??\n",
    "\n",
    "# 3.\n",
    "fdp_BF_3c       = ??\n",
    "fdp_hochberg_3c = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30866f29-da92-4abe-b94a-45ef3f2ab6a8",
   "metadata": {},
   "source": [
    "### Task 3d)\n",
    "\n",
    "We are now going to conduct simulations in order to investigate the $FWER$, $FDR$ and the power of each signal variable in the model setup of Task 3b. We do this in a double loop. \n",
    "\n",
    "The inner loop generates 1000 vectors of simulated outputs and records (using a vector of indicator variables) which of the input variables in `X_3a` are significant at a 15% significance level. The underlying p-values are to be corrected using the Bonferroni and Benjamini-Hochberg corrections. \n",
    "\n",
    "The outer loop runs through 20 cases with signal variables of increasing signal strength. Signal strength is controlled by multiplying the coefficient vector `beta_sim_3a` of your model  with a magnitude factor `mag` whose 20 possible values are saved in a vector `magnitudes`. After the inner loop has been run, the outer loop uses information of variable selection in every simulated dataset to calculate FDR and FWER, excluding the pvalue of a test for statistical significance of the intercept. The outer loop also stores the power of significance test for coefficients on each of the four signal variables in the model (i.e. those with nonzero beta coefficient).\n",
    "\n",
    "*Note*: The code chunk below prepares a number of empty vectors and matrices that are to be filled in the inner and outer loops. The matrices starting with `selected.` might be a bit confusing. Keep in mind that they have dimension $\\mathrm{sim} \\times p$. So each iteration of the inner loop is supposed to fill one of their rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d8e014-d96f-443a-baf7-4ededcf05658",
   "metadata": {
    "tags": [
     "code_chunk_10"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_10\n",
    "# Don't change anything here:\n",
    "np.random.seed(4456)\n",
    "\n",
    "sim = 1000\n",
    "p = X_3a.shape[1]\n",
    "magnitudes = np.linspace(0, 1, 20)\n",
    "\n",
    "Power_Hochberg_3d    = np.zeros((len(magnitudes), len(signal_index_3a)))\n",
    "Power_BF_3d          = np.zeros((len(magnitudes), len(signal_index_3a)))\n",
    "FWER_Hochberg_3d     = np.zeros(len(magnitudes))\n",
    "FWER_BF_3d           = np.zeros(len(magnitudes))\n",
    "FDR_BF               = np.zeros(len(magnitudes))\n",
    "FDR_Hochberg         = np.zeros(len(magnitudes))\n",
    "selected_BF_3d       = np.zeros((sim, p))\n",
    "selected_Hochberg_3d = np.zeros((sim, p))\n",
    "\n",
    "# Start changing stuff below\n",
    "for i, mag in enumerate(magnitudes):\n",
    "    for ii in range(sim):\n",
    "        beta_loop_3d          = ??\n",
    "        y_loop_3d             = ??\n",
    "        pvals_loop_3d         = ??\n",
    "        p_BF_loop_3d          = ??\n",
    "        p_hoch_loop_3d        = ??\n",
    "        selected_BF_3d[ii, :] = ??\n",
    "        selected_Hochberg_3d[ii, :] = ??\n",
    "    \n",
    "    Power_BF_3d[i, :]       = ??\n",
    "    Power_Hochberg_3d[i, :] = ??\n",
    "    FWER_BF_3d[i]           = ??\n",
    "    FWER_Hochberg_3d[i]     = ??\n",
    "    FDR_BF[i]               = ??\n",
    "    FDR_Hochberg[i]         = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5bce1e-7b04-4c14-9696-a2302dd2d6a6",
   "metadata": {},
   "source": [
    "### Task 3e)\n",
    "For the variable with the strongest signal (i.e. the largest coefficient in `beta_sim_3a`) create a line plot that plots the power of a significance test on its coefficient against the signal strength `mag`. Since you have power for both Bonferroni amd Benjamini-Hochberg adjustments, you need to include the corresponding two power curves in the same plot.\n",
    "\n",
    "Then, create another plot in which you repeat the same task for the weakest signal (i.e. the variable with smallest nonzero coefficient in `beta_sim_3a`).\n",
    "\n",
    "Why is the difference in power between Bonferroni and Benjamini-Hochberg adjusted p-values larger on the weakest signal? Write your answer into the string variable `why_difference_3e`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fdd9d-1338-421d-87e0-5ff4c12382f5",
   "metadata": {
    "tags": [
     "code_chunk_11"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_11\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "??\n",
    "\n",
    "why_difference_3e = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd2b88-0811-4185-8b20-85d892e826ed",
   "metadata": {},
   "source": [
    "#### Task 3f)\n",
    "\n",
    "Now plot the FWER rates for both Bonferroni and Benamini-Hochberg adjustments against the signal strength `mag`. \n",
    "Which pattern can you see in these lines? Write your answer into the string variable `what_pattern_3f`. \n",
    "What causes these patterns? Write your answer into the string variable `why_pattern_3f`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60098e8f-e63d-42d0-9490-c7e3c11c7f8d",
   "metadata": {
    "tags": [
     "code_chunk_12"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_12\n",
    "??\n",
    "\n",
    "what_pattern_3f = \"??\"\n",
    "why_pattern_3f  = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6303a-7180-4fc7-998d-dee91ebc855b",
   "metadata": {},
   "source": [
    "## Part 4: Knock-off inference\n",
    "\n",
    "In Mullainathan, Sendhil, and Jann Spiess. 2017. \"Machine Learning: An Applied Econometric Approach.\" *Journal of Economic Perspectives*, 31 (2): 87-106, the authors compared several ML techniques for prediction of house prices (log of house prices). Here we will explore this data sets for variable selection (slightly cleaned by us a priori).\n",
    "First we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete this before uploading\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9aee62-af44-4d55-94a7-2dbb60259d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ahs_data = pd.read_csv(\"asst7_data_part4.csv\")\n",
    "y_4a = ahs_data['y_4a'].to_numpy()\n",
    "X_4a = ahs_data.iloc[:,1:]\n",
    "X_varnames_4a = X_4a.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf4cf7-4a75-4595-884b-c6cd30042b22",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "As usual when working with lasso, we rescale the variances of all predictors and demean all variables before training any model. Do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dad7e2d-2373-4d34-9855-7377e66453ac",
   "metadata": {
    "tags": [
     "code_chunk_13"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_13\n",
    "scaler_4a = StandardScaler()\n",
    "X_4a      = scaler_4a.fit_transform(X_4a)\n",
    "y_4a      = y_4a - y_4a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69f68e0-14fb-4ecc-a70c-ba6679f09dc8",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "Now use *scikit-learn* to perform ten-fold cross-validation and save the resulting object as `lasso_cv_4b`.\n",
    "\n",
    "I have already written some code that saves the tuning parameter value selected through the one-S.E. rule as `alpha_1se`. Train a lasso model with this regularization parameter value and save this model as `lasso_fit_4b`. \n",
    "\n",
    "Lastly, extract the names of the selected variables (i.e. the variables corresponding to the non-zero coefficients of the lasso fit with chosen value for the regularization parameter $\\lambda$) into an object `selected_names_4b` and save the total number of selected variables as `num_selected_4b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c527a5-d115-4e7d-9e8a-2f2f8c2202eb",
   "metadata": {
    "tags": [
     "code_chunk_14"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: ['PHONEother' 'KITCHEN2' 'MOBILTYPother' 'WINTEROVENother'\n",
      " 'WINTERELSPother' 'NEWC1' 'DISH2' 'WASH2' 'DRY2' 'REFR2' 'BATHS' 'BEDRMS'\n",
      " 'DENS' 'FAMRM' 'HALFB' 'KITCH' 'LIVING' 'RECRM' 'BUILT' 'LOT' 'UNITSF'\n",
      " 'CLIMB' 'DIRAC' 'AIRSYS2' 'WELDUSother' 'STEAM1' 'STEAM2' 'FRPLother'\n",
      " 'FPLWK2' 'FPINSother' 'DISPL2' 'TRASH2' 'TYPEother' 'ENOEAPP2' 'ECNTAIR1'\n",
      " 'ECNTAIRother' 'EAIRCother' 'EHEATUT2' 'EHEATUTother' 'EFRIDGE2'\n",
      " 'EWASHR2' 'EWASHRother' 'EDISHWR2' 'EDISHWRother' 'ETRSHCP1' 'ETRSHCP2'\n",
      " 'AIR' 'NUMAIR' 'WATERother' 'WATERDother' 'HOTPIP2' 'SEWDISTPother'\n",
      " 'SEWDUSother' 'KEXCLUother' 'INCPother' 'BUSPER' 'EXCLUS' 'LAUNDY'\n",
      " 'OTHRUN' 'DRSHOP2' 'CONDO3' 'CELLAR4' 'CELLARother' 'WHNGET'\n",
      " 'FRSTOCother' 'PREOCCother' 'EBARother' 'OTBUP1' 'OTBUP2' 'NUNITS'\n",
      " 'PLUGSother' 'OWNLOTother' 'ROOMS' 'PLUMB2' 'ZADEQother' 'LEAKother'\n",
      " 'WTRHRLother' 'RATSother' 'EGOODother' 'HOWH' 'BSINK2' 'TOILET2'\n",
      " 'ELEVWK1' 'EROACH2' 'EVROD2' 'ROACHFRQother' 'CRACKS2' 'EBOARDother'\n",
      " 'EBROKEother' 'ECRUMBother' 'EHOLERother' 'EMISSRother' 'EMISSWother'\n",
      " 'ESAGRother' 'HOLES2' 'IFBLOWother' 'FREEZEother' 'IFDRYother'\n",
      " 'IFSEWother' 'NUMCOLD' 'NUMSEW' 'OTHCLDother' 'NOWIREother' 'REGION2'\n",
      " 'REGION3' 'REGION4' 'METRO7' 'METROother' 'UNITSFMISSother' 'EXCLUSMISS0'\n",
      " 'HOWHMISSother' 'NUMTLTMISS0']\n",
      "Number of Selected Features: 112\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_14\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Implement cross-validation\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=5)\n",
    "lasso_cv_4b = LassoCV(cv=cv, random_state=5)\n",
    "lasso_cv_4b.fit(X_4a, y_4a)\n",
    "\n",
    "# Get tuning parameter obtained with one-S.E. rule.\n",
    "mean_mse      = np.mean(lasso_cv_4b.mse_path_, axis=1)\n",
    "stderr_mse    = np.std(lasso_cv_4b.mse_path_, axis=1)\n",
    "idx_alpha_1se = np.where(mean_mse <= (min(mean_mse) + stderr_mse))[0][-1]\n",
    "alpha_1se     = lasso_cv_4b.alphas_[idx_alpha_1se]\n",
    "\n",
    "# Train Lasso model with optimal tuning parameter \n",
    "lasso_fit_4b = Lasso(alpha=alpha_1se)\n",
    "lasso_fit_4b.fit(X_4a, y_4a)\n",
    "\n",
    "# Extract non-zero coefficients\n",
    "coef_nonzero      = np.where(lasso_fit_4b.coef_ != 0)[0]\n",
    "\n",
    "selected_names_4b = np.array(X_varnames_4a)[coef_nonzero]\n",
    "num_selected_4b   = len(coef_nonzero)\n",
    "\n",
    "print(\"Selected Feature Names:\", selected_names_4b)\n",
    "print(\"Number of Selected Features:\", num_selected_4b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026747fd-a250-40f6-a79b-277020b2226b",
   "metadata": {},
   "source": [
    "### Task 4c) \n",
    "Next, we do controlled variable selection using knock-offs. In python, this is implemented via the `knockpy` package can be accessed through PyPI.\n",
    "The central fucntion for conducting knock-off variable selection is `KnockoffFilter()`, whose [documentation](https://amspector100.github.io/knockpy/apiref.html) provides you with an overview of its capabilities.\n",
    "\n",
    "First, call `KnockoffFilter()` to specify the setup of our variable selection exercise. More specifically, we want to base  \n",
    "The statistic that we want to use to do variable selection is\n",
    "$$\n",
    "Z_i  =max \\{\\lambda:\\beta_j(\\lambda) \\neq 0\\},\n",
    "$$\n",
    "as suggested in the seminal paper of Candes and Barber (2015). We also use the fixed-X option for knockoffs. Save the resulting specification object as `knockoff_filter_4c`.\n",
    "\n",
    "Second, apply the `forward()`-method to `knockoff_filter_4c` to run the knockoff filter. I order to select the correct arguments, recall that we want to select predictors of log houes prices `y_4a` from the whose set of predictor candidates `X_4a`. Now, we want to control the probability of making a mistake with an FDR of 15%.\n",
    "\n",
    "Lastly, save the names of the selected variables in an object `selected_names_4c` and the number of selected variables as `number_selected_4c`. How does the number selected variables in Task 4b compare to the number that you have now? Write you answer into the string variable `num_selected_compare_4c`.\n",
    "\n",
    "*Note:* The *knockpy* package deos not deal particularly well with large datasets. As a result, you might get a memory error. In this (likely) case, use fewer data points of `X_4a` and `y_4a`. To a certain degree, you can also free up unused memory using the `collect()` function in the `gc` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "242f92b8-09b3-4d7f-8389-e3b0a5921407",
   "metadata": {
    "tags": [
     "code_chunk_15"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: ['MOBILTYPother' 'WINTEROVENother' 'NEWC1' 'DISH2' 'DRY2' 'BATHS' 'BEDRMS'\n",
      " 'DENS' 'HALFB' 'KITCH' 'LIVING' 'BUILT' 'LOT' 'UNITSF' 'CLIMB' 'AIRSYS2'\n",
      " 'WELDUSother' 'FPLWK2' 'FPINSother' 'DISPL2' 'TRASH2' 'TYPEother'\n",
      " 'ENOEAPP2' 'EHEATUT2' 'EFRIDGE2' 'EWASHR2' 'EDISHWR2' 'EDISHWRother'\n",
      " 'AIR' 'NUMAIR' 'WATERDother' 'SEWDUSother' 'INCPother' 'BUSPER' 'LAUNDY'\n",
      " 'CONDO3' 'CELLAR4' 'CELLARother' 'WHNGET' 'FRSTOCother' 'PREOCCother'\n",
      " 'EBARother' 'OTBUP1' 'OTBUP2' 'OWNLOTother' 'ROOMS' 'ZADEQother'\n",
      " 'WTRHRLother' 'RATSother' 'EGOODother' 'HOWH' 'BSINK2' 'TOILET2'\n",
      " 'ELEVWK1' 'EVROD2' 'ROACHFRQother' 'CRACKS2' 'HOLES2' 'FREEZEother'\n",
      " 'IFSEWother' 'OTHCLDother' 'NOWIREother' 'REGION2' 'REGION3' 'REGION4'\n",
      " 'METROother' 'UNITSFMISSother' 'HOWHMISSother' 'NUMTLTMISS0']\n",
      "Number of Selected Features: 69\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_15\n",
    "import knockpy\n",
    "#Even after clearing memory my desktop could only handle 85% of the dataset\n",
    "np.random.seed(5)\n",
    "n_rows = X_4a.shape[0]\n",
    "subset_idx = np.random.choice(n_rows, size=int(0.85*n_rows), replace=False)\n",
    "X_small = X_4a[subset_idx, :]\n",
    "y_small = y_4a[subset_idx]\n",
    "# 1.\n",
    "knockoff_filter_4c = knockpy.KnockoffFilter(\n",
    "    fstat=\"lasso\", \n",
    "    ksampler=\"fx\"\n",
    ")\n",
    "\n",
    "# 2.\n",
    "knockoff_result_4c = knockoff_filter_4c.forward(X_small, y_small, fdr=0.15)\n",
    "selected_names_4c = np.array(X_varnames_4a)[knockoff_result_4c.astype(int)==1]\n",
    "number_selected_4c = selected_names_4c.size\n",
    "\n",
    "print(\"Selected Feature Names:\", selected_names_4c)\n",
    "print(\"Number of Selected Features:\", number_selected_4c)\n",
    "\n",
    "# 3. \n",
    "num_selected_compare_4c = \"Compared to 112 of 117 features selected in 4b, we now only have 69 features selected.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAN51",
   "language": "python",
   "name": "stan51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
