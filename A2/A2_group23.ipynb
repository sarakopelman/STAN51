{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff03aa7d-85fe-44e9-ad19-19c845b9457c",
   "metadata": {
    "id": "ff03aa7d-85fe-44e9-ad19-19c845b9457c"
   },
   "source": [
    "# DABN13 - Assignment 2\n",
    "## Preamble: Predicting purchases in online shops.\n",
    "This assignment will be based on a dataset on online shopper purchase data which is available on the UC Irvine Machine Learning Library. A description of all variables is available [here  ](https://www.kaggle.com/henrysue/online-shoppers-intention). Among the 11 variables in the dataset, we will only use the three following:\n",
    "\n",
    " - **Revenue**: (TRUE/FALSE) Whether a purchase was made by a visitor to the online shop.\n",
    " - **ProductRelated_Duration**: (numerical) Time spend on pages relevant/related to the product in question.\n",
    " - **ExitRates**: (numerical) The percentage of visits to the online shop that end with visiting the site of the product at issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6312ad4d-9bb5-4c6b-9aca-73068d5e3d83",
   "metadata": {
    "id": "6312ad4d-9bb5-4c6b-9aca-73068d5e3d83"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyreadr as prdr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ff2b82-e37e-470d-a55f-16116de150c0",
   "metadata": {
    "id": "90ff2b82-e37e-470d-a55f-16116de150c0"
   },
   "source": [
    "## Part 1: Logistic regression with `sklearn`\n",
    "\n",
    "In this basic part, we are getting some experience with using scikit-learn to learn logistic regressions. In the steps below, we will train a quite small logistic regression model with `Revenue` as output variable and the following inputs (in addition to the intercept):\n",
    "\n",
    "1. ` ExitRate ` without further transformation\n",
    "2. The (natural) logarithm of ` ProductRelated_Duration + 1 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17e9b0-43f6-490f-b292-2def4224a950",
   "metadata": {
    "id": "9a17e9b0-43f6-490f-b292-2def4224a950"
   },
   "source": [
    "### Task 1a)\n",
    "First, we need to prepare our data. Conduct the following steps:\n",
    "\n",
    "1. Load the data into python and save it in an object called `shoppers`. The dataset is contained in a comma-separated spreadsheet. Accordingly, you will need to use the `read_csv()` command in Pandas.\n",
    "2. Use the `columns` method on `shoppers` to change the variable name of `ExitRates` to `ER`. More specifically, you will need to rename a particular column of `shoppers`.\n",
    "3. Create a new variable `lPR_Dur` inside `shoppers` that contains the (natural) logarithm of  `ProductRelated_Duration + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c854c8-f1a6-4ff5-adb0-845ee7cf5cad",
   "metadata": {
    "id": "47c854c8-f1a6-4ff5-adb0-845ee7cf5cad",
    "tags": [
     "code_chunk_01"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_01\n",
    "# os.chdir('C:\\\\Users\\\\??') # Change working directory if needed\n",
    "\n",
    "# 1.\n",
    "shoppers = pd.read_csv('online_shoppers_intention.csv')\n",
    "\n",
    "# 2.\n",
    "shoppers = shoppers.rename(columns={'ExitRates': 'ER'})\n",
    "\n",
    "# 3.\n",
    "shoppers['lPR_Dur'] = np.log(shoppers['ProductRelated_Duration']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7f533-7912-45b8-a8e0-3536a6bd0dcc",
   "metadata": {
    "id": "ecc7f533-7912-45b8-a8e0-3536a6bd0dcc"
   },
   "source": [
    "### Task 1b)\n",
    "\n",
    "Technically, we could use the `statsmodels` package to learn a logistic regressions. However, the functionality of `statsmodels` addresses applications in classical statistics rather than machine learning. Since we want to use logistic regression as a supervised learning algorithm in order to predict the output of new data points, it is much wiser to switch to scikit-learn (`sklearn`). `sklearn` provides you with simple functions that implement model training, tuning and validation and therefore covers the entire spectrum of standard methods in supervised learning.\n",
    "\n",
    "Learning a logistic regression with `sklearn` works almost identically as with `statsmodels`. Do it as follows:\n",
    "\n",
    "1. Create a  Pandas series `y_1b` that contains the `Revenue` variable from `shoppers`\n",
    "2. Create a similar data frame `X_1b` whose columns are  ` ER `, ` lPR_Dur ` and the square of `lPR_Dur`.\n",
    "3. Instantiate a logistic regression model using the `LogisticRegression()` function from the `linear_model` module and save this model specification as `glm_spec_1b`. The function has an argument `penalty`. Set this argument to `None`. Furthermore ensure that an intercept is automatically added to the input variables.\n",
    "4. Apply the `fit()`-method to `glm_spec_1b` to learn the model and save the learned model as `glm_fit_1b`. Here, specify that the model is learned using with inputs `X_1b` and output `y_1b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4a8d78-597a-459f-bc78-0c38fd3f3d8a",
   "metadata": {
    "id": "4b4a8d78-597a-459f-bc78-0c38fd3f3d8a",
    "tags": [
     "code_chunk_02"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-32.63987528  -0.58940172   0.06451171]]\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_02\n",
    "# 1.\n",
    "y_1b = pd.Series(shoppers[\"Revenue\"])\n",
    "\n",
    "# 2.\n",
    "X_1b = pd.DataFrame({\n",
    "    'ER': shoppers['ER'],\n",
    "    'lPR_Dur': shoppers['lPR_Dur'],\n",
    "    'lPr_Dur2': shoppers['lPR_Dur']**2\n",
    "})\n",
    "\n",
    "# 3.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "glm_spec_1b = LogisticRegression(penalty = None, fit_intercept=True)\n",
    "\n",
    "# 4.\n",
    "glm_fit_1b  = glm_spec_1b.fit(X_1b, y_1b)\n",
    "\n",
    "print(glm_fit_1b.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb034-a558-4904-b5ae-613e5a03e175",
   "metadata": {
    "id": "36efb034-a558-4904-b5ae-613e5a03e175"
   },
   "source": [
    "### Task 1c)\n",
    "A fundamental principle of machine learning is that we divide the data available to us into different sets which we use for learning, model tuning and algorithm choice.\n",
    "\n",
    "`sklearn` makes this very simple for us since its `model_selection`-module contains the `train_test_split()` function. It returns splitted versions of all objects that you provide as inputs. I prepared a preliminary code chunk below that shows how you need to specify the desired splitted objects. Complete the code chunk by entering the inputs to `train_test_split()`. More specifically, provide\n",
    "\n",
    "1. the data objects before splitting in correct order,\n",
    "2. an argument that allocates 50% of all data points to the training data,\n",
    "3. an argument that sets the initial state of the random number generator to 3 (we need this to ensure replicability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3ddaf8-10e3-4bb2-860a-3986e89c4187",
   "metadata": {
    "id": "eb3ddaf8-10e3-4bb2-860a-3986e89c4187",
    "tags": [
     "code_chunk_03"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_03\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_1b_train, X_1b_test, y_1b_train, y_1b_test = train_test_split(X_1b, y_1b,train_size=0.5, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effd7a7e-7be6-459a-918d-6fae1dcde2f2",
   "metadata": {
    "id": "effd7a7e-7be6-459a-918d-6fae1dcde2f2"
   },
   "source": [
    "### Task 1d)\n",
    "Now, please refit the model from Task 1b using only your training data from Task 1c. Save the resulting learned model as `glm_fit_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "befc1502-1faf-436b-8624-575f88426474",
   "metadata": {
    "id": "befc1502-1faf-436b-8624-575f88426474",
    "tags": [
     "code_chunk_04"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-32.23959758  -0.68440669   0.07091558]]\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_04\n",
    "glm_fit_1d =  glm_spec_1b.fit(X_1b_train, y_1b_train)\n",
    "\n",
    "print(glm_fit_1d.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa53b29-6bd5-40c0-a880-7580c943dba6",
   "metadata": {
    "id": "2fa53b29-6bd5-40c0-a880-7580c943dba6"
   },
   "source": [
    "### Task 1e)\n",
    "Now that we have fitted our model, we want to evaluate its predictive performance on hold-out validation data. We have already created this test data in Task 1c as `y_1b_test` and `X_1b_test.`\n",
    "\n",
    "Before we can evaluate model performance, we first need to obtain predicted probabilities for purchases on the test data. Use the ` predict_proba() `-method on `glm_fit_1d` to obtain such predicted conditional probabilities from the model fit in Task 1d on the observations in our test set. Save them as `glm_prob_1e`\n",
    "\n",
    "According to the sklearn documentation, the columns of `glm_prob_1e` contain class probabilities where classes are ordered as they are in `glm_fit_1d.classes_`. Which column index contains the probabilities for class `True`? Write your answer into the string variable `which_column_truepreds_1e`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d0b8b-edc7-469d-96ab-f085dae9eff6",
   "metadata": {
    "id": "b34d0b8b-edc7-469d-96ab-f085dae9eff6",
    "tags": [
     "code_chunk_05"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_05\n",
    "glm_prob_1e = glm_fit_1d.predict_proba(X_1b_test)\n",
    "which_column_truepreds_1e = \"The probabilities are contained in the second column, so column 1 (since python is 0-indexed)-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324b30-692c-426c-bd3c-45f27dac2795",
   "metadata": {
    "id": "68324b30-692c-426c-bd3c-45f27dac2795"
   },
   "source": [
    "### Task 1f)\n",
    "In a next step, we apply a classification rule to map our predicted probabilities into class predictions. Our classification rule is to predict the most likely class.\n",
    "\n",
    "First, create a new vector ` glm_pred_1f ` which has as many elements as ` glm_prob_1e ` and which consists entirely of the logical statement `False` (without citation marks!)\n",
    "\n",
    "Second, replace `False` in ` glm_pred_1f ` with `True` for all elements where the corresponding predicted probability for category `True` exceeds the threshold used for the classifier mentioned above. You can do this by indexing `glm_pred_1f` using square brackets. Simply write a true-or-false (or logical) statement in the square brackets. For rows where it is true, the value of ` glm_pred_1f ` will be changed.\n",
    "\n",
    "Please additionally write the true-or-false statement that you use into the string variable ` logical_1f ` for the sake of making assignment evaluation simpler for us.\n",
    "\n",
    "*Note:* We could even get class predictions by applying the `predict()`-method to `glm_fit_1d`. However, that does not allow us to fully control the classification rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2427d51e-44ba-4161-9aa2-06271d84e182",
   "metadata": {
    "id": "2427d51e-44ba-4161-9aa2-06271d84e182",
    "tags": [
     "code_chunk_06"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False,  True]), array([6150,   15]))\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_06\n",
    "# 1.\n",
    "glm_pred_1f = np.zeros(glm_prob_1e.shape[0], dtype=bool)\n",
    "\n",
    "# 2.\n",
    "glm_pred_1f[glm_prob_1e[:,1]>0.5] = True\n",
    "\n",
    "print(np.unique(glm_pred_1f, return_counts=True)) # Run this without modification\n",
    "\n",
    "# 3.\n",
    "logical_1f = \"glm_prob_1e[:,1]>0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa8cef-d6a3-4822-97cb-5c46cd8097d1",
   "metadata": {
    "id": "0dfa8cef-d6a3-4822-97cb-5c46cd8097d1"
   },
   "source": [
    "### Task 1g)\n",
    "Choose an appropriate error function and write its name in the string variable `chosenerrfun_1g`. Then, use the objects created in the previous tasks of this part to obtain (overall) test error for the logistic regression model fitted in Task 1d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d24da15-606a-4cef-82ad-16ae14da6c6e",
   "metadata": {
    "id": "0d24da15-606a-4cef-82ad-16ae14da6c6e",
    "tags": [
     "code_chunk_07"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15506893755068937\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_07\n",
    "chosenerrfun_1g = \"Missclassification rate\"\n",
    "testerr_1g      = np.mean(glm_pred_1f != y_1b_test)\n",
    "print(testerr_1g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e0142-c9f6-4e87-b998-bd092842e92a",
   "metadata": {
    "id": "338e0142-c9f6-4e87-b998-bd092842e92a"
   },
   "source": [
    "## Part 2: Class-specific prediction errors\n",
    "\n",
    "This part is more advanced than parts 1 and 3. In classification problems, overall test error may not always be our primary concern. To get a more differentiated picture, confusion matrices and the ROC curve are useful tools. We will get both using the metric module of Scikit-learn which provides a large number of performance criteria for binary classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1debb5b-e443-400a-845d-b5b67c4c34d0",
   "metadata": {
    "id": "f1debb5b-e443-400a-845d-b5b67c4c34d0"
   },
   "source": [
    "### Task 2a)\n",
    "A basic confusion matrix can easily be obtained using the `confusion_matrix` function of `sklearn.metrics`. This function only requires two inputs:\n",
    "\n",
    "1. The test outcomes.\n",
    "2. The predicted class on the test set.\n",
    "\n",
    "Obtain the confusion matrix and save it as `confumat_2a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aaa594-ff61-4f8c-b8b8-448b9e3ecbc7",
   "metadata": {
    "id": "82aaa594-ff61-4f8c-b8b8-448b9e3ecbc7",
    "tags": [
     "code_chunk_08"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_08\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confumat_2a = ??\n",
    "print(confumat_2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d7315-4754-4637-bad2-90715cc2b8cc",
   "metadata": {
    "id": "4f8d7315-4754-4637-bad2-90715cc2b8cc"
   },
   "source": [
    "### Task 2b)\n",
    "The confusion matrix obtained in the last task is rather rudimentary. For this reason, we are now going to write a function which produces a more luxurious confusion matrix with additional performance measures.\n",
    "\n",
    "Below, I prepared a function that takes a true outcomes, predicted probabilities for the category of interest and a desired threshold probability for the classification rule as inputs and returns a dictionary object containing the corresponding confusion matrix, TPR, FPR and overall classification error. All that is left for you is the following tasks:\n",
    "\n",
    "1. Specify the row and column names of the confusion matrix correctly\n",
    "2. Use the four elements of `cmat` to calculate `FPR`, `TPR` and the classification error `error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c23f2-24e8-45e6-b871-f189b96b97a9",
   "metadata": {
    "id": "a72c23f2-24e8-45e6-b871-f189b96b97a9",
    "tags": [
     "code_chunk_09"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_09\n",
    "def  performancemetrics(y_true, y_prob, cutoff):\n",
    "    # Don't change the 2 rows below\n",
    "    y_pred       = np.greater(y_prob,cutoff)\n",
    "    cmat         = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "    # 1.\n",
    "    column_names = [\"??\", \"??\"]\n",
    "    row_names    = [\"??\", \"??\"]\n",
    "\n",
    "    # 2.\n",
    "    FPR   = ??\n",
    "    TPR   = ??\n",
    "    error = ??\n",
    "\n",
    "    # Don't change the two lines below\n",
    "    cmat         = pd.DataFrame(cmat, columns=column_names, index=row_names)\n",
    "    allresults = {'confusion_matrix': cmat, 'FPR':FPR, \"TPR\":TPR, 'classification_error':error}\n",
    "    return(allresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d2641-3b7a-4f39-baed-254eaa8d2a19",
   "metadata": {
    "id": "436d2641-3b7a-4f39-baed-254eaa8d2a19"
   },
   "source": [
    "### Task 2c)\n",
    "\n",
    "Now save as `metrics_2c` the output of `performancemetrics()` with the prediction object of task 2a and a threshold probability of 50% as input. Additionally, answer two questions:\n",
    "\n",
    "1. Are you satisfied with the overall accuracy with which our model predicts purchases?\n",
    "2. Is the accuracy with which observed purchases are correctly predicted satisfactory? Assume here that we have considerable interest in predicting actual purchases correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2316ca-c3d5-4627-b76e-885e5ee8fd5b",
   "metadata": {
    "id": "7c2316ca-c3d5-4627-b76e-885e5ee8fd5b",
    "tags": [
     "code_chunk_10"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_10\n",
    "\n",
    "metrics_2c = ??\n",
    "print(metrics_2c)\n",
    "overall_acc_verdict2c      = \"??\"\n",
    "obs_purchase_acc_verdict2c = \"??\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8e85a-d080-4767-ad6b-a9ea49984797",
   "metadata": {
    "id": "ede8e85a-d080-4767-ad6b-a9ea49984797"
   },
   "source": [
    "### Task 2d)\n",
    "\n",
    "Assume we would like to get a classifier that has relatively balanced class-specific performance. In other words, we want to choose a threshold such that TPR is approximately 1-FPR. In order to see the trade-offs that are available to us, we will look at a ROC curve.\n",
    "\n",
    "ROC curves can be plotted using the `RocCurveDisplay.from_predictions()` function in the sklearn metrics module. The inputs to this function are\n",
    "\n",
    "1. The test outcomes.\n",
    "2. The predicted probabilities for class 1 on the test set.\n",
    "\n",
    "Use the code chunk below to plot a ROC curve. Then proceed with the rest of this task in the following text block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd638b2-9087-4ea6-b97e-42f28e6527c5",
   "metadata": {
    "id": "2bd638b2-9087-4ea6-b97e-42f28e6527c5",
    "tags": [
     "code_chunk_11"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_11\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fd071-e433-4b7d-82a6-f7ae9ac67ba0",
   "metadata": {
    "id": "996fd071-e433-4b7d-82a6-f7ae9ac67ba0"
   },
   "source": [
    "Unfortunately, the plotted ROC curve does not indicate the threshold value leading to a particular point on the curve. Fortunately, the information contained in a ROC curve can be obtained using the `roc_curve()` function in the sklearn metrics module.\n",
    "\n",
    "The inputs to  `roc_curve()` are true outputs and the predicted probabilities for class 1. Use the corresponding series from Part 1 of this assignment as function inputs and save the result as `roc_fpr_2d`, `roc_tpr_2d` and `roc_thresholds_2d`.\n",
    "\n",
    "Next, find the the threshold that balances class-specific performance best. You could do that by finding the index at which TPR and (1-FPR) are as close are possible. Then you simply return the value of `roc_thresholds_2d` at this index. State your chosen threshold in the  variable `opt_threshold_2d`. Round to two decimals, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd9f1e-9478-4064-a260-92ede98f9a14",
   "metadata": {
    "id": "c7cd9f1e-9478-4064-a260-92ede98f9a14",
    "tags": [
     "code_chunk_12"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_12\n",
    "from sklearn.metrics import ??\n",
    "\n",
    "# 1.\n",
    "roc_fpr_2d, roc_tpr_2d, roc_thresholds_2d = roc_curve(??)\n",
    "\n",
    "# 2.\n",
    "opt_threshold_2d = ??\n",
    "\n",
    "print(opt_threshold_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3741fe7-a150-4ad2-b775-96e28809e13a",
   "metadata": {
    "id": "c3741fe7-a150-4ad2-b775-96e28809e13a"
   },
   "source": [
    "### Task 2e)\n",
    "To what extend does our chosen threshold from Task 2d compromise overall accuracy? To find this out, use your `performancemetrics()` function from Task 2b and save the output that you get with the threshold chosen in Task 2d as `metrics_2e`.\n",
    "\n",
    "Then, use the string variable `is_accuracy_compromised_2e` to discuss whether overall accuracy is noteably affected if we switch from a threshold probability of 50% to a value that balances class-specific prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e8b82-ac50-40fa-a2ff-0087412ff7de",
   "metadata": {
    "id": "f00e8b82-ac50-40fa-a2ff-0087412ff7de",
    "tags": [
     "code_chunk_13"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_13\n",
    "metrics_2e = ??\n",
    "print(metrics_2e)\n",
    "\n",
    "is_accuracy_compromised_2e = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d68ce5-28c7-472b-a1e3-07f1dc5dfc79",
   "metadata": {
    "id": "d1d68ce5-28c7-472b-a1e3-07f1dc5dfc79"
   },
   "source": [
    "## Part 3: Multiclass logistic regression with `sklearn`\n",
    "\n",
    "We can use the `LogisticRegression` function from `sklearn` to learn models whose output variable has more than two categories. The data we are using to learn a multiclass logistic regression is drug consumption data. Our response variable is usage of drugs (Cocaine , Crack, Ecstasy, and Heroin) and we have three possible responses, \"never used\", \"used more than a year ago\", and \"used within a year\". As explanatory variables we have personality test data, demographic data, and consumption of chocolate, alcohol, and nicotine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c0298-afb6-4b7d-b32d-15610d67931f",
   "metadata": {
    "id": "c44c0298-afb6-4b7d-b32d-15610d67931f"
   },
   "source": [
    "### Task 3a)\n",
    "\n",
    "Conduct the following steps to prepare your data:\n",
    "\n",
    "1. Load `drug_train.RDS` using the `read_r()` function in the `pyreadr` package and save it as a data frame `drug_data`.\n",
    "2. Extract the variable *drugs.usage* into a Pandas series called `y_3a`\n",
    "3. Create a Pandas dataframe `X_3a` containing all other variables in `drug_data`.  Note that you will need to transform categorical variables into indicator variables, e.g. using the `get_dummies()` function in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27776fed-bcd9-4b01-8ba2-ec14d9d4b846",
   "metadata": {
    "id": "27776fed-bcd9-4b01-8ba2-ec14d9d4b846",
    "tags": [
     "code_chunk_14"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_14\n",
    "# 1.\n",
    "drug_data = prdr.read_r(??)\n",
    "drug_data = drug_data[None]\n",
    "\n",
    "# 2.\n",
    "y_3a = ??\n",
    "\n",
    "# 3.\n",
    "X_3a = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f2eb6-7279-431a-8341-a052389e69d5",
   "metadata": {
    "id": "ab1f2eb6-7279-431a-8341-a052389e69d5"
   },
   "source": [
    "### Task 3b)\n",
    "\n",
    "Specify and learn fit a multinomial logistic regression using `LogisticRegression()`. In the specification, choose no penalty and make sure an intercept is added. Additionally, set the `multi_class` argument to `'multinomial'`. Lastly, increase the number of iterations if this is needed for the learning algorithm to converge.\n",
    "\n",
    "Save the resulting object as `mlogit_fit_3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be9547-16aa-4004-9282-c14e267f3909",
   "metadata": {
    "id": "d1be9547-16aa-4004-9282-c14e267f3909",
    "tags": [
     "code_chunk_15"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_15\n",
    "mlogit_fit_3b = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c430dd66-b606-40fd-abae-a24c19ef96f8",
   "metadata": {
    "id": "c430dd66-b606-40fd-abae-a24c19ef96f8"
   },
   "source": [
    "### Task 3c)\n",
    "\n",
    "We now skip the entire data splitting step that we discussed in Part 1. Instead, we say that model `mlogit_fit_3b` is our result of the entire modeling process and it is implemented in practice. We now get new test data for which we obtain output predictions. To prepare this, conduct the following steps:\n",
    "\n",
    "1. Load *drug_test.RDS* as object `drug_data_test`.\n",
    "2. Create an input variable matrix `X_3c_test` in the same way as in Task 3a, but using `drug_data_test` instead of `drug_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d2f7d-b881-42f2-a9d5-090762b3f50a",
   "metadata": {
    "id": "333d2f7d-b881-42f2-a9d5-090762b3f50a",
    "tags": [
     "code_chunk_16"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_16\n",
    "drug_data_test = ??\n",
    "drug_data_test = drug_data_test[None]\n",
    "\n",
    "X_3c_test = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865f1abd-e837-4994-bd43-a3d696534fb3",
   "metadata": {
    "id": "865f1abd-e837-4994-bd43-a3d696534fb3"
   },
   "source": [
    "### Task 3d)\n",
    "\n",
    "Get predictions from model `mlogit_fit_3b` for your test data using the `predict_proba()` and `predict()` methods. Save your predictions as `mlogit_prob_3c` and `mlogit_pred_3c`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ef853-029d-4121-ae35-a576c33a219b",
   "metadata": {
    "id": "b95ef853-029d-4121-ae35-a576c33a219b",
    "tags": [
     "code_chunk_17"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_17\n",
    "mlogit_prob_3c = ??\n",
    "mlogit_pred_3c = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fcdca-e2af-426c-b9fe-c7b266f9a02b",
   "metadata": {
    "id": "461fcdca-e2af-426c-b9fe-c7b266f9a02b"
   },
   "source": [
    "As time passes, you also observe outputs for your first batch of test data. This allows you to evaluate the accuracy of your prediction model during deployment. The variable *drugs.usage* in `drug_data_test` contains these test outputs. Extract them into a vector `ytest_3d`.\n",
    "\n",
    "Then, use the `confusion_matrix` function in sklearn to get a confusion matrix. Save it as `confumat_3d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1acef3-5c16-49bd-aa10-0064c0ee2be7",
   "metadata": {
    "id": "be1acef3-5c16-49bd-aa10-0064c0ee2be7",
    "tags": [
     "code_chunk_18"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_18\n",
    "ytest_3d    = ??\n",
    "confumat_3d = ??"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
