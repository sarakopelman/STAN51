{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1cb77-5344-418c-94b3-7bacb800c746",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 6\n",
    "\n",
    "## Preamble: Data\n",
    "In this lab we are using a dataset on beer purchases. Our goal is to predict if light beer purchased in the US is BUD light. To achieve this goal, we will use the information provided by the following socioeconomic characteristics:\n",
    "* market           - where the beer is bought\n",
    "* buyertype        - who is the buyer () \n",
    "* income           - ranges of income\n",
    "* childrenUnder6   - does the buyer have children under 6 years old\n",
    "* children6to17    - does the buyer have children between 6 and 17\n",
    "* age              - bracketed age groups\n",
    "* employment       - fully employed, partially employed, no employment.\n",
    "* degree           - level of occupation\n",
    "* occuptation      - which sector you are employed in\n",
    "* ethnic           - white, asian, hispanic, black or other\n",
    "* microwave        - own a microwave\n",
    "* dishwasher       - own a dishwasher\n",
    "* tvcable          - what type cable tv subscription you have\n",
    "* singlefamilyhome - are you in a single family home\n",
    "* npeople          - number of people you live with 1,2,3,4, +5\n",
    "\n",
    "First, we load the dataset and create an output variable that indicates purchases of Bud Light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "620cb558-0655-4ff6-80d8-51cc89e3c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# os.chdir(\"?\") Change working directory if needed \n",
    "\n",
    "lb    = pd.read_csv(\"LightBeer2.csv\")\n",
    "y     = np.zeros(shape=lb.shape[0])\n",
    "y[lb['beer_brand'] == \"BUD LIGHT\"]     = 1\n",
    "demog = lb.iloc[:,9:]\n",
    "demog = pd.get_dummies(demog, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174fd7-d779-4207-a30e-224c89bdcbc9",
   "metadata": {},
   "source": [
    "We also split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a5c287b-7069-426c-b3c1-15857ff123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(demog, y, train_size=0.75, shuffle=False)\n",
    "\n",
    "stdz_X = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = stdz_X.transform(X_train)\n",
    "X_test  = stdz_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983d48-57e5-4342-9935-16676fd3a106",
   "metadata": {},
   "source": [
    "## Part 1: Specifying and training neural networks\n",
    "We will now start building a neural network to predict the purchase of Bud Light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7cab-178e-4fee-b942-8ab68a140e23",
   "metadata": {},
   "source": [
    "### Task 1a) \n",
    "We start with specifying the architecture of our very first and very small neural net `model1`.\n",
    "Add three layers to `model1`, two hidden layers with $30$ and $15$ hidden units, respectively, and an output layer.\n",
    "For the two hidden layers you should use the ReLU activation function. Additionally, choose a suitable activation for the output layer, given that we have a classification problem. See [the Keras documentation](https://keras.io/api/layers/activations/) for activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7d9388d-cbd6-4d10-90ff-2c3d974a8238",
   "metadata": {
    "tags": [
     "code_chunk_01"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_01\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "keras.utils.set_random_seed(3)\n",
    "# Initialize a first model\n",
    "model1 = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model1.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "model1.add(layers.Dense(30, activation= \"relu\"))\n",
    "\n",
    "model1.add(layers.Dense(15, activation= \"relu\"))\n",
    "\n",
    "model1.add(layers.Dense(1, activation = \"sigmoid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd214-22bf-42fa-a973-2c27f0643431",
   "metadata": {},
   "source": [
    "### Task 1b) \n",
    "\n",
    "Next, we compile our model specification. From [https://keras.io/api/losses/probabilistic_losses/](losses) select a suitable loss function for our classification problem. As optimization algorithm use *Adam* with learning rate $0.00003$. Lastly, use `accuracy` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a88ddd5-3e85-4238-96a3-81b914f4fc9f",
   "metadata": {
    "tags": [
     "code_chunk_02"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_02\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Compile the model\n",
    "myopt = Adam(learning_rate = 0.00003)\n",
    "\n",
    "model1.compile(loss = \"binary_crossentropy\", optimizer = myopt, metrics = [\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2904463-9a53-4ea3-b175-adfc9ff0f599",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "Now train the model using $250$ epochs, a batch_size of $2^8$, and use $25\\%$ of the data for validation.\n",
    "Use the string variable `loss_valloss_difference_1c` to describe and explain the observed difference between validation loss and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0f00f52b-92c7-4100-bfe0-05cdd6f0fe63",
   "metadata": {
    "tags": [
     "code_chunk_03"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8155 - loss: 0.4200 - val_accuracy: 0.6229 - val_loss: 0.6929\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8157 - loss: 0.4197 - val_accuracy: 0.6229 - val_loss: 0.6937\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8158 - loss: 0.4193 - val_accuracy: 0.6228 - val_loss: 0.6944\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8158 - loss: 0.4190 - val_accuracy: 0.6232 - val_loss: 0.6952\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8162 - loss: 0.4186 - val_accuracy: 0.6221 - val_loss: 0.6960\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8163 - loss: 0.4183 - val_accuracy: 0.6217 - val_loss: 0.6968\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.4179 - val_accuracy: 0.6217 - val_loss: 0.6975\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8165 - loss: 0.4176 - val_accuracy: 0.6214 - val_loss: 0.6983\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8167 - loss: 0.4172 - val_accuracy: 0.6207 - val_loss: 0.6990\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8168 - loss: 0.4169 - val_accuracy: 0.6208 - val_loss: 0.6997\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8171 - loss: 0.4166 - val_accuracy: 0.6210 - val_loss: 0.7004\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8175 - loss: 0.4162 - val_accuracy: 0.6210 - val_loss: 0.7011\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8178 - loss: 0.4159 - val_accuracy: 0.6211 - val_loss: 0.7018\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8180 - loss: 0.4156 - val_accuracy: 0.6197 - val_loss: 0.7026\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.4152 - val_accuracy: 0.6197 - val_loss: 0.7033\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8184 - loss: 0.4149 - val_accuracy: 0.6181 - val_loss: 0.7041\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8188 - loss: 0.4146 - val_accuracy: 0.6204 - val_loss: 0.7049\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8188 - loss: 0.4143 - val_accuracy: 0.6186 - val_loss: 0.7055\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8190 - loss: 0.4140 - val_accuracy: 0.6186 - val_loss: 0.7063\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8190 - loss: 0.4136 - val_accuracy: 0.6186 - val_loss: 0.7069\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8193 - loss: 0.4133 - val_accuracy: 0.6186 - val_loss: 0.7075\n",
      "Epoch 22/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8195 - loss: 0.4130 - val_accuracy: 0.6167 - val_loss: 0.7082\n",
      "Epoch 23/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8196 - loss: 0.4127 - val_accuracy: 0.6167 - val_loss: 0.7088\n",
      "Epoch 24/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8196 - loss: 0.4124 - val_accuracy: 0.6167 - val_loss: 0.7094\n",
      "Epoch 25/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8197 - loss: 0.4121 - val_accuracy: 0.6164 - val_loss: 0.7099\n",
      "Epoch 26/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8199 - loss: 0.4118 - val_accuracy: 0.6169 - val_loss: 0.7106\n",
      "Epoch 27/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8204 - loss: 0.4115 - val_accuracy: 0.6166 - val_loss: 0.7111\n",
      "Epoch 28/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.4112 - val_accuracy: 0.6177 - val_loss: 0.7117\n",
      "Epoch 29/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8211 - loss: 0.4109 - val_accuracy: 0.6181 - val_loss: 0.7124\n",
      "Epoch 30/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8213 - loss: 0.4106 - val_accuracy: 0.6175 - val_loss: 0.7130\n",
      "Epoch 31/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8217 - loss: 0.4103 - val_accuracy: 0.6175 - val_loss: 0.7136\n",
      "Epoch 32/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8220 - loss: 0.4101 - val_accuracy: 0.6175 - val_loss: 0.7143\n",
      "Epoch 33/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8221 - loss: 0.4098 - val_accuracy: 0.6171 - val_loss: 0.7148\n",
      "Epoch 34/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8223 - loss: 0.4095 - val_accuracy: 0.6270 - val_loss: 0.7155\n",
      "Epoch 35/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8225 - loss: 0.4092 - val_accuracy: 0.6270 - val_loss: 0.7160\n",
      "Epoch 36/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8226 - loss: 0.4089 - val_accuracy: 0.6260 - val_loss: 0.7166\n",
      "Epoch 37/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8229 - loss: 0.4086 - val_accuracy: 0.6260 - val_loss: 0.7171\n",
      "Epoch 38/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8229 - loss: 0.4083 - val_accuracy: 0.6258 - val_loss: 0.7176\n",
      "Epoch 39/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8229 - loss: 0.4080 - val_accuracy: 0.6262 - val_loss: 0.7181\n",
      "Epoch 40/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.4077 - val_accuracy: 0.6262 - val_loss: 0.7187\n",
      "Epoch 41/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8233 - loss: 0.4074 - val_accuracy: 0.6248 - val_loss: 0.7193\n",
      "Epoch 42/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.4072 - val_accuracy: 0.6246 - val_loss: 0.7197\n",
      "Epoch 43/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8236 - loss: 0.4069 - val_accuracy: 0.6243 - val_loss: 0.7202\n",
      "Epoch 44/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8239 - loss: 0.4066 - val_accuracy: 0.6241 - val_loss: 0.7207\n",
      "Epoch 45/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8241 - loss: 0.4063 - val_accuracy: 0.6240 - val_loss: 0.7213\n",
      "Epoch 46/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8242 - loss: 0.4060 - val_accuracy: 0.6241 - val_loss: 0.7217\n",
      "Epoch 47/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8243 - loss: 0.4058 - val_accuracy: 0.6240 - val_loss: 0.7222\n",
      "Epoch 48/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8244 - loss: 0.4055 - val_accuracy: 0.6240 - val_loss: 0.7227\n",
      "Epoch 49/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8246 - loss: 0.4052 - val_accuracy: 0.6240 - val_loss: 0.7231\n",
      "Epoch 50/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8247 - loss: 0.4050 - val_accuracy: 0.6237 - val_loss: 0.7236\n",
      "Epoch 51/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8249 - loss: 0.4047 - val_accuracy: 0.6237 - val_loss: 0.7241\n",
      "Epoch 52/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8250 - loss: 0.4044 - val_accuracy: 0.6237 - val_loss: 0.7246\n",
      "Epoch 53/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8251 - loss: 0.4042 - val_accuracy: 0.6238 - val_loss: 0.7253\n",
      "Epoch 54/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8252 - loss: 0.4039 - val_accuracy: 0.6173 - val_loss: 0.7258\n",
      "Epoch 55/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8254 - loss: 0.4037 - val_accuracy: 0.6189 - val_loss: 0.7262\n",
      "Epoch 56/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8254 - loss: 0.4034 - val_accuracy: 0.6190 - val_loss: 0.7269\n",
      "Epoch 57/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8254 - loss: 0.4031 - val_accuracy: 0.6190 - val_loss: 0.7275\n",
      "Epoch 58/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8255 - loss: 0.4029 - val_accuracy: 0.6190 - val_loss: 0.7281\n",
      "Epoch 59/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8255 - loss: 0.4026 - val_accuracy: 0.6192 - val_loss: 0.7286\n",
      "Epoch 60/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8256 - loss: 0.4024 - val_accuracy: 0.6192 - val_loss: 0.7292\n",
      "Epoch 61/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8260 - loss: 0.4021 - val_accuracy: 0.6175 - val_loss: 0.7297\n",
      "Epoch 62/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8261 - loss: 0.4019 - val_accuracy: 0.6175 - val_loss: 0.7303\n",
      "Epoch 63/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8263 - loss: 0.4016 - val_accuracy: 0.6157 - val_loss: 0.7309\n",
      "Epoch 64/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8265 - loss: 0.4014 - val_accuracy: 0.6157 - val_loss: 0.7315\n",
      "Epoch 65/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8266 - loss: 0.4011 - val_accuracy: 0.6111 - val_loss: 0.7321\n",
      "Epoch 66/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8269 - loss: 0.4009 - val_accuracy: 0.6111 - val_loss: 0.7326\n",
      "Epoch 67/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8271 - loss: 0.4006 - val_accuracy: 0.6111 - val_loss: 0.7331\n",
      "Epoch 68/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8272 - loss: 0.4004 - val_accuracy: 0.6111 - val_loss: 0.7339\n",
      "Epoch 69/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8273 - loss: 0.4001 - val_accuracy: 0.6105 - val_loss: 0.7344\n",
      "Epoch 70/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8274 - loss: 0.3999 - val_accuracy: 0.6105 - val_loss: 0.7350\n",
      "Epoch 71/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8275 - loss: 0.3996 - val_accuracy: 0.6105 - val_loss: 0.7355\n",
      "Epoch 72/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8275 - loss: 0.3994 - val_accuracy: 0.6105 - val_loss: 0.7360\n",
      "Epoch 73/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8276 - loss: 0.3991 - val_accuracy: 0.6111 - val_loss: 0.7365\n",
      "Epoch 74/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8277 - loss: 0.3989 - val_accuracy: 0.6111 - val_loss: 0.7369\n",
      "Epoch 75/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.3987 - val_accuracy: 0.6107 - val_loss: 0.7375\n",
      "Epoch 76/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8278 - loss: 0.3984 - val_accuracy: 0.6108 - val_loss: 0.7380\n",
      "Epoch 77/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3982 - val_accuracy: 0.6108 - val_loss: 0.7385\n",
      "Epoch 78/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3980 - val_accuracy: 0.6095 - val_loss: 0.7390\n",
      "Epoch 79/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3977 - val_accuracy: 0.6095 - val_loss: 0.7395\n",
      "Epoch 80/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3975 - val_accuracy: 0.6094 - val_loss: 0.7400\n",
      "Epoch 81/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3973 - val_accuracy: 0.6093 - val_loss: 0.7406\n",
      "Epoch 82/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3970 - val_accuracy: 0.6092 - val_loss: 0.7412\n",
      "Epoch 83/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3968 - val_accuracy: 0.6091 - val_loss: 0.7418\n",
      "Epoch 84/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3966 - val_accuracy: 0.6081 - val_loss: 0.7423\n",
      "Epoch 85/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3963 - val_accuracy: 0.6065 - val_loss: 0.7429\n",
      "Epoch 86/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3961 - val_accuracy: 0.6065 - val_loss: 0.7434\n",
      "Epoch 87/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.3959 - val_accuracy: 0.6067 - val_loss: 0.7439\n",
      "Epoch 88/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.3957 - val_accuracy: 0.6067 - val_loss: 0.7444\n",
      "Epoch 89/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8281 - loss: 0.3954 - val_accuracy: 0.6066 - val_loss: 0.7450\n",
      "Epoch 90/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8282 - loss: 0.3952 - val_accuracy: 0.6063 - val_loss: 0.7454\n",
      "Epoch 91/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8284 - loss: 0.3950 - val_accuracy: 0.6063 - val_loss: 0.7458\n",
      "Epoch 92/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.3948 - val_accuracy: 0.6063 - val_loss: 0.7464\n",
      "Epoch 93/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.3945 - val_accuracy: 0.6063 - val_loss: 0.7468\n",
      "Epoch 94/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.3943 - val_accuracy: 0.6004 - val_loss: 0.7472\n",
      "Epoch 95/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.3941 - val_accuracy: 0.6004 - val_loss: 0.7476\n",
      "Epoch 96/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8287 - loss: 0.3939 - val_accuracy: 0.6006 - val_loss: 0.7479\n",
      "Epoch 97/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8289 - loss: 0.3937 - val_accuracy: 0.6006 - val_loss: 0.7484\n",
      "Epoch 98/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8291 - loss: 0.3935 - val_accuracy: 0.6006 - val_loss: 0.7489\n",
      "Epoch 99/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.3932 - val_accuracy: 0.6005 - val_loss: 0.7492\n",
      "Epoch 100/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.3930 - val_accuracy: 0.6005 - val_loss: 0.7499\n",
      "Epoch 101/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.3928 - val_accuracy: 0.5957 - val_loss: 0.7504\n",
      "Epoch 102/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8294 - loss: 0.3926 - val_accuracy: 0.5957 - val_loss: 0.7507\n",
      "Epoch 103/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.3924 - val_accuracy: 0.6022 - val_loss: 0.7511\n",
      "Epoch 104/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.3922 - val_accuracy: 0.6022 - val_loss: 0.7516\n",
      "Epoch 105/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.3920 - val_accuracy: 0.6022 - val_loss: 0.7520\n",
      "Epoch 106/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8299 - loss: 0.3918 - val_accuracy: 0.6023 - val_loss: 0.7525\n",
      "Epoch 107/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.3916 - val_accuracy: 0.6023 - val_loss: 0.7529\n",
      "Epoch 108/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.3914 - val_accuracy: 0.6025 - val_loss: 0.7533\n",
      "Epoch 109/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8300 - loss: 0.3911 - val_accuracy: 0.6025 - val_loss: 0.7539\n",
      "Epoch 110/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.3909 - val_accuracy: 0.6023 - val_loss: 0.7543\n",
      "Epoch 111/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8305 - loss: 0.3907 - val_accuracy: 0.6022 - val_loss: 0.7547\n",
      "Epoch 112/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8306 - loss: 0.3905 - val_accuracy: 0.6023 - val_loss: 0.7552\n",
      "Epoch 113/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8307 - loss: 0.3903 - val_accuracy: 0.6023 - val_loss: 0.7557\n",
      "Epoch 114/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.3901 - val_accuracy: 0.6023 - val_loss: 0.7561\n",
      "Epoch 115/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8309 - loss: 0.3899 - val_accuracy: 0.6023 - val_loss: 0.7566\n",
      "Epoch 116/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8310 - loss: 0.3897 - val_accuracy: 0.6032 - val_loss: 0.7571\n",
      "Epoch 117/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8310 - loss: 0.3895 - val_accuracy: 0.6053 - val_loss: 0.7575\n",
      "Epoch 118/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8312 - loss: 0.3893 - val_accuracy: 0.6085 - val_loss: 0.7580\n",
      "Epoch 119/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8312 - loss: 0.3891 - val_accuracy: 0.6085 - val_loss: 0.7585\n",
      "Epoch 120/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8313 - loss: 0.3889 - val_accuracy: 0.6085 - val_loss: 0.7589\n",
      "Epoch 121/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8315 - loss: 0.3887 - val_accuracy: 0.6083 - val_loss: 0.7594\n",
      "Epoch 122/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8319 - loss: 0.3885 - val_accuracy: 0.6083 - val_loss: 0.7599\n",
      "Epoch 123/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8321 - loss: 0.3883 - val_accuracy: 0.6083 - val_loss: 0.7604\n",
      "Epoch 124/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8323 - loss: 0.3881 - val_accuracy: 0.6084 - val_loss: 0.7609\n",
      "Epoch 125/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8324 - loss: 0.3879 - val_accuracy: 0.6084 - val_loss: 0.7613\n",
      "Epoch 126/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8326 - loss: 0.3877 - val_accuracy: 0.6084 - val_loss: 0.7617\n",
      "Epoch 127/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8327 - loss: 0.3875 - val_accuracy: 0.6084 - val_loss: 0.7622\n",
      "Epoch 128/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8329 - loss: 0.3873 - val_accuracy: 0.6080 - val_loss: 0.7627\n",
      "Epoch 129/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8329 - loss: 0.3871 - val_accuracy: 0.6079 - val_loss: 0.7630\n",
      "Epoch 130/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8332 - loss: 0.3869 - val_accuracy: 0.6077 - val_loss: 0.7637\n",
      "Epoch 131/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8333 - loss: 0.3867 - val_accuracy: 0.6077 - val_loss: 0.7641\n",
      "Epoch 132/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8333 - loss: 0.3865 - val_accuracy: 0.6062 - val_loss: 0.7646\n",
      "Epoch 133/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8333 - loss: 0.3863 - val_accuracy: 0.6056 - val_loss: 0.7651\n",
      "Epoch 134/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8334 - loss: 0.3861 - val_accuracy: 0.6051 - val_loss: 0.7657\n",
      "Epoch 135/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8335 - loss: 0.3859 - val_accuracy: 0.6058 - val_loss: 0.7663\n",
      "Epoch 136/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8337 - loss: 0.3857 - val_accuracy: 0.6063 - val_loss: 0.7667\n",
      "Epoch 137/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8337 - loss: 0.3855 - val_accuracy: 0.6056 - val_loss: 0.7671\n",
      "Epoch 138/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8338 - loss: 0.3853 - val_accuracy: 0.6056 - val_loss: 0.7677\n",
      "Epoch 139/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8337 - loss: 0.3851 - val_accuracy: 0.6056 - val_loss: 0.7681\n",
      "Epoch 140/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8339 - loss: 0.3849 - val_accuracy: 0.6056 - val_loss: 0.7686\n",
      "Epoch 141/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8339 - loss: 0.3847 - val_accuracy: 0.6056 - val_loss: 0.7692\n",
      "Epoch 142/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8340 - loss: 0.3845 - val_accuracy: 0.6054 - val_loss: 0.7696\n",
      "Epoch 143/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8340 - loss: 0.3843 - val_accuracy: 0.6047 - val_loss: 0.7700\n",
      "Epoch 144/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8341 - loss: 0.3841 - val_accuracy: 0.6047 - val_loss: 0.7705\n",
      "Epoch 145/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8341 - loss: 0.3839 - val_accuracy: 0.6047 - val_loss: 0.7710\n",
      "Epoch 146/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.3837 - val_accuracy: 0.6047 - val_loss: 0.7713\n",
      "Epoch 147/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.3835 - val_accuracy: 0.6047 - val_loss: 0.7719\n",
      "Epoch 148/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8344 - loss: 0.3833 - val_accuracy: 0.6047 - val_loss: 0.7723\n",
      "Epoch 149/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.3831 - val_accuracy: 0.6043 - val_loss: 0.7728\n",
      "Epoch 150/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.3829 - val_accuracy: 0.6042 - val_loss: 0.7734\n",
      "Epoch 151/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8344 - loss: 0.3827 - val_accuracy: 0.6042 - val_loss: 0.7738\n",
      "Epoch 152/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8344 - loss: 0.3825 - val_accuracy: 0.6101 - val_loss: 0.7741\n",
      "Epoch 153/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.3823 - val_accuracy: 0.6101 - val_loss: 0.7745\n",
      "Epoch 154/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.3821 - val_accuracy: 0.6100 - val_loss: 0.7748\n",
      "Epoch 155/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.3819 - val_accuracy: 0.6127 - val_loss: 0.7753\n",
      "Epoch 156/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8346 - loss: 0.3817 - val_accuracy: 0.6129 - val_loss: 0.7756\n",
      "Epoch 157/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8347 - loss: 0.3815 - val_accuracy: 0.6131 - val_loss: 0.7759\n",
      "Epoch 158/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8347 - loss: 0.3813 - val_accuracy: 0.6130 - val_loss: 0.7763\n",
      "Epoch 159/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8347 - loss: 0.3811 - val_accuracy: 0.6130 - val_loss: 0.7768\n",
      "Epoch 160/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.3809 - val_accuracy: 0.6130 - val_loss: 0.7772\n",
      "Epoch 161/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.3807 - val_accuracy: 0.6130 - val_loss: 0.7776\n",
      "Epoch 162/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.3805 - val_accuracy: 0.6129 - val_loss: 0.7779\n",
      "Epoch 163/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8348 - loss: 0.3804 - val_accuracy: 0.6101 - val_loss: 0.7783\n",
      "Epoch 164/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8349 - loss: 0.3802 - val_accuracy: 0.6102 - val_loss: 0.7785\n",
      "Epoch 165/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8352 - loss: 0.3800 - val_accuracy: 0.6103 - val_loss: 0.7789\n",
      "Epoch 166/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8353 - loss: 0.3798 - val_accuracy: 0.6104 - val_loss: 0.7792\n",
      "Epoch 167/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8355 - loss: 0.3796 - val_accuracy: 0.6104 - val_loss: 0.7794\n",
      "Epoch 168/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8357 - loss: 0.3794 - val_accuracy: 0.6104 - val_loss: 0.7797\n",
      "Epoch 169/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8358 - loss: 0.3792 - val_accuracy: 0.6104 - val_loss: 0.7801\n",
      "Epoch 170/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8361 - loss: 0.3791 - val_accuracy: 0.6108 - val_loss: 0.7805\n",
      "Epoch 171/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8362 - loss: 0.3789 - val_accuracy: 0.6108 - val_loss: 0.7808\n",
      "Epoch 172/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8362 - loss: 0.3787 - val_accuracy: 0.6108 - val_loss: 0.7810\n",
      "Epoch 173/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8365 - loss: 0.3785 - val_accuracy: 0.6108 - val_loss: 0.7814\n",
      "Epoch 174/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8365 - loss: 0.3783 - val_accuracy: 0.6094 - val_loss: 0.7817\n",
      "Epoch 175/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8364 - loss: 0.3782 - val_accuracy: 0.6093 - val_loss: 0.7820\n",
      "Epoch 176/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8365 - loss: 0.3780 - val_accuracy: 0.6093 - val_loss: 0.7824\n",
      "Epoch 177/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8366 - loss: 0.3778 - val_accuracy: 0.6097 - val_loss: 0.7827\n",
      "Epoch 178/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8367 - loss: 0.3776 - val_accuracy: 0.6097 - val_loss: 0.7831\n",
      "Epoch 179/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8368 - loss: 0.3774 - val_accuracy: 0.6090 - val_loss: 0.7836\n",
      "Epoch 180/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8371 - loss: 0.3773 - val_accuracy: 0.6089 - val_loss: 0.7840\n",
      "Epoch 181/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8373 - loss: 0.3771 - val_accuracy: 0.6089 - val_loss: 0.7844\n",
      "Epoch 182/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8375 - loss: 0.3769 - val_accuracy: 0.6080 - val_loss: 0.7848\n",
      "Epoch 183/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8377 - loss: 0.3767 - val_accuracy: 0.6080 - val_loss: 0.7852\n",
      "Epoch 184/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8379 - loss: 0.3766 - val_accuracy: 0.6080 - val_loss: 0.7855\n",
      "Epoch 185/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8381 - loss: 0.3764 - val_accuracy: 0.6080 - val_loss: 0.7857\n",
      "Epoch 186/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8383 - loss: 0.3762 - val_accuracy: 0.6061 - val_loss: 0.7861\n",
      "Epoch 187/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8383 - loss: 0.3760 - val_accuracy: 0.6061 - val_loss: 0.7865\n",
      "Epoch 188/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.3759 - val_accuracy: 0.6060 - val_loss: 0.7869\n",
      "Epoch 189/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8386 - loss: 0.3757 - val_accuracy: 0.6051 - val_loss: 0.7872\n",
      "Epoch 190/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8386 - loss: 0.3755 - val_accuracy: 0.6051 - val_loss: 0.7875\n",
      "Epoch 191/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8388 - loss: 0.3754 - val_accuracy: 0.6055 - val_loss: 0.7878\n",
      "Epoch 192/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8388 - loss: 0.3752 - val_accuracy: 0.6055 - val_loss: 0.7881\n",
      "Epoch 193/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8387 - loss: 0.3750 - val_accuracy: 0.6055 - val_loss: 0.7886\n",
      "Epoch 194/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8391 - loss: 0.3749 - val_accuracy: 0.6055 - val_loss: 0.7888\n",
      "Epoch 195/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.3747 - val_accuracy: 0.5990 - val_loss: 0.7894\n",
      "Epoch 196/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8392 - loss: 0.3745 - val_accuracy: 0.5995 - val_loss: 0.7897\n",
      "Epoch 197/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8393 - loss: 0.3743 - val_accuracy: 0.5993 - val_loss: 0.7901\n",
      "Epoch 198/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8394 - loss: 0.3742 - val_accuracy: 0.5992 - val_loss: 0.7904\n",
      "Epoch 199/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8395 - loss: 0.3740 - val_accuracy: 0.6025 - val_loss: 0.7907\n",
      "Epoch 200/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8394 - loss: 0.3738 - val_accuracy: 0.6019 - val_loss: 0.7911\n",
      "Epoch 201/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8395 - loss: 0.3737 - val_accuracy: 0.6019 - val_loss: 0.7916\n",
      "Epoch 202/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8396 - loss: 0.3735 - val_accuracy: 0.6029 - val_loss: 0.7919\n",
      "Epoch 203/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8397 - loss: 0.3733 - val_accuracy: 0.6019 - val_loss: 0.7924\n",
      "Epoch 204/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8397 - loss: 0.3732 - val_accuracy: 0.6025 - val_loss: 0.7927\n",
      "Epoch 205/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8397 - loss: 0.3730 - val_accuracy: 0.6035 - val_loss: 0.7930\n",
      "Epoch 206/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8398 - loss: 0.3728 - val_accuracy: 0.6019 - val_loss: 0.7934\n",
      "Epoch 207/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8399 - loss: 0.3727 - val_accuracy: 0.6016 - val_loss: 0.7936\n",
      "Epoch 208/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8400 - loss: 0.3725 - val_accuracy: 0.6011 - val_loss: 0.7940\n",
      "Epoch 209/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8401 - loss: 0.3723 - val_accuracy: 0.6011 - val_loss: 0.7944\n",
      "Epoch 210/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8402 - loss: 0.3722 - val_accuracy: 0.6167 - val_loss: 0.7948\n",
      "Epoch 211/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8403 - loss: 0.3720 - val_accuracy: 0.6167 - val_loss: 0.7951\n",
      "Epoch 212/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8404 - loss: 0.3718 - val_accuracy: 0.6167 - val_loss: 0.7954\n",
      "Epoch 213/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8404 - loss: 0.3717 - val_accuracy: 0.6170 - val_loss: 0.7958\n",
      "Epoch 214/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.3715 - val_accuracy: 0.6170 - val_loss: 0.7961\n",
      "Epoch 215/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.3713 - val_accuracy: 0.6170 - val_loss: 0.7965\n",
      "Epoch 216/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8406 - loss: 0.3712 - val_accuracy: 0.6170 - val_loss: 0.7969\n",
      "Epoch 217/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.3710 - val_accuracy: 0.6159 - val_loss: 0.7973\n",
      "Epoch 218/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8410 - loss: 0.3709 - val_accuracy: 0.6157 - val_loss: 0.7976\n",
      "Epoch 219/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.3707 - val_accuracy: 0.6157 - val_loss: 0.7980\n",
      "Epoch 220/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.3705 - val_accuracy: 0.6157 - val_loss: 0.7983\n",
      "Epoch 221/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8415 - loss: 0.3704 - val_accuracy: 0.6157 - val_loss: 0.7987\n",
      "Epoch 222/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8417 - loss: 0.3702 - val_accuracy: 0.6157 - val_loss: 0.7990\n",
      "Epoch 223/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8419 - loss: 0.3701 - val_accuracy: 0.6157 - val_loss: 0.7993\n",
      "Epoch 224/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.3699 - val_accuracy: 0.6157 - val_loss: 0.7994\n",
      "Epoch 225/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.3697 - val_accuracy: 0.6159 - val_loss: 0.7998\n",
      "Epoch 226/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8422 - loss: 0.3696 - val_accuracy: 0.6159 - val_loss: 0.8001\n",
      "Epoch 227/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8424 - loss: 0.3694 - val_accuracy: 0.6159 - val_loss: 0.8002\n",
      "Epoch 228/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8425 - loss: 0.3693 - val_accuracy: 0.6160 - val_loss: 0.8007\n",
      "Epoch 229/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8428 - loss: 0.3691 - val_accuracy: 0.6162 - val_loss: 0.8010\n",
      "Epoch 230/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8429 - loss: 0.3690 - val_accuracy: 0.6080 - val_loss: 0.8014\n",
      "Epoch 231/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8429 - loss: 0.3688 - val_accuracy: 0.6054 - val_loss: 0.8017\n",
      "Epoch 232/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8430 - loss: 0.3687 - val_accuracy: 0.6048 - val_loss: 0.8020\n",
      "Epoch 233/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8434 - loss: 0.3685 - val_accuracy: 0.6046 - val_loss: 0.8023\n",
      "Epoch 234/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.3683 - val_accuracy: 0.6025 - val_loss: 0.8027\n",
      "Epoch 235/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8440 - loss: 0.3682 - val_accuracy: 0.6024 - val_loss: 0.8028\n",
      "Epoch 236/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8442 - loss: 0.3680 - val_accuracy: 0.6024 - val_loss: 0.8032\n",
      "Epoch 237/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.3679 - val_accuracy: 0.6024 - val_loss: 0.8035\n",
      "Epoch 238/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8445 - loss: 0.3677 - val_accuracy: 0.6023 - val_loss: 0.8040\n",
      "Epoch 239/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8447 - loss: 0.3676 - val_accuracy: 0.6038 - val_loss: 0.8042\n",
      "Epoch 240/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8446 - loss: 0.3674 - val_accuracy: 0.6038 - val_loss: 0.8047\n",
      "Epoch 241/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8447 - loss: 0.3673 - val_accuracy: 0.6044 - val_loss: 0.8048\n",
      "Epoch 242/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8448 - loss: 0.3671 - val_accuracy: 0.6041 - val_loss: 0.8054\n",
      "Epoch 243/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8449 - loss: 0.3669 - val_accuracy: 0.6041 - val_loss: 0.8058\n",
      "Epoch 244/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8450 - loss: 0.3668 - val_accuracy: 0.6041 - val_loss: 0.8062\n",
      "Epoch 245/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8451 - loss: 0.3666 - val_accuracy: 0.6041 - val_loss: 0.8065\n",
      "Epoch 246/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8452 - loss: 0.3665 - val_accuracy: 0.6041 - val_loss: 0.8071\n",
      "Epoch 247/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8453 - loss: 0.3663 - val_accuracy: 0.6041 - val_loss: 0.8075\n",
      "Epoch 248/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8454 - loss: 0.3662 - val_accuracy: 0.6041 - val_loss: 0.8078\n",
      "Epoch 249/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.3660 - val_accuracy: 0.6041 - val_loss: 0.8085\n",
      "Epoch 250/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.3659 - val_accuracy: 0.6042 - val_loss: 0.8088\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_03\n",
    "nn_trainlog1 = model1.fit(x = X_train, y = y_train,\n",
    "                          epochs = 250,\n",
    "                          batch_size = 256,\n",
    "                          validation_split= 0.25)\n",
    "\n",
    "\n",
    "loss_valloss_difference_1c = \"The training loss is the loss computed on the training data. It will keep decreasing as we make the model fit closer and closer to the training data. \" \\\n",
    "\"Validation loss on the other hand is the loss computed on a separate dataset that the model has not seen during training. It provides an estimate of how well the model generalizes to unseen data. \" \\\n",
    "\"As training progresses, the validation loss usually decreases at first, but if the model starts overfitting to the training data, the validation starts increasing even though the training loss keeps decreasing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189128-f787-4b8c-8c96-6b782065dad7",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "In Lecture 9 we used early stopping to avoid overfitting. Apply this here, with `patience` argument set to 20. Create a new model, `model1b`, which otherwise should have a setup identical to `model1`. In which epoch did the model training procedure stop? Figure this out by counting the number of elements in `model1b_fit.history['loss']` and write your answer into the string variable `when_earlystop_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13f2cb-b29b-4288-9b8d-e69f331faff4",
   "metadata": {
    "tags": [
     "code_chunk_04"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4575 - loss: 0.7629 - val_accuracy: 0.3981 - val_loss: 0.8086\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4948 - loss: 0.7287 - val_accuracy: 0.5218 - val_loss: 0.7280\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5377 - loss: 0.7063 - val_accuracy: 0.6065 - val_loss: 0.6731\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5627 - loss: 0.6911 - val_accuracy: 0.6696 - val_loss: 0.6363\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5892 - loss: 0.6800 - val_accuracy: 0.7237 - val_loss: 0.6120\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6043 - loss: 0.6710 - val_accuracy: 0.7391 - val_loss: 0.5958\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6142 - loss: 0.6635 - val_accuracy: 0.7475 - val_loss: 0.5846\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6252 - loss: 0.6569 - val_accuracy: 0.7593 - val_loss: 0.5766\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6321 - loss: 0.6510 - val_accuracy: 0.7643 - val_loss: 0.5708\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6396 - loss: 0.6458 - val_accuracy: 0.7750 - val_loss: 0.5665\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6434 - loss: 0.6410 - val_accuracy: 0.7812 - val_loss: 0.5633\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6468 - loss: 0.6366 - val_accuracy: 0.7831 - val_loss: 0.5604\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6492 - loss: 0.6325 - val_accuracy: 0.7955 - val_loss: 0.5581\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6544 - loss: 0.6285 - val_accuracy: 0.7918 - val_loss: 0.5566\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6622 - loss: 0.6249 - val_accuracy: 0.7887 - val_loss: 0.5558\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6669 - loss: 0.6215 - val_accuracy: 0.7888 - val_loss: 0.5553\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 0.6182 - val_accuracy: 0.7941 - val_loss: 0.5549\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6766 - loss: 0.6150 - val_accuracy: 0.7926 - val_loss: 0.5549\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6810 - loss: 0.6120 - val_accuracy: 0.7893 - val_loss: 0.5548\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6820 - loss: 0.6091 - val_accuracy: 0.7860 - val_loss: 0.5546\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6826 - loss: 0.6064 - val_accuracy: 0.7889 - val_loss: 0.5544\n",
      "Epoch 22/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6834 - loss: 0.6038 - val_accuracy: 0.7875 - val_loss: 0.5543\n",
      "Epoch 23/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6840 - loss: 0.6014 - val_accuracy: 0.7807 - val_loss: 0.5544\n",
      "Epoch 24/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6868 - loss: 0.5990 - val_accuracy: 0.7804 - val_loss: 0.5547\n",
      "Epoch 25/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6881 - loss: 0.5968 - val_accuracy: 0.7855 - val_loss: 0.5551\n",
      "Epoch 26/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6900 - loss: 0.5947 - val_accuracy: 0.7861 - val_loss: 0.5553\n",
      "Epoch 27/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6902 - loss: 0.5927 - val_accuracy: 0.7860 - val_loss: 0.5557\n",
      "Epoch 28/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.5908 - val_accuracy: 0.7813 - val_loss: 0.5561\n",
      "Epoch 29/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6922 - loss: 0.5889 - val_accuracy: 0.7761 - val_loss: 0.5564\n",
      "Epoch 30/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6926 - loss: 0.5870 - val_accuracy: 0.7688 - val_loss: 0.5567\n",
      "Epoch 31/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6936 - loss: 0.5853 - val_accuracy: 0.7659 - val_loss: 0.5569\n",
      "Epoch 32/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6947 - loss: 0.5835 - val_accuracy: 0.7659 - val_loss: 0.5571\n",
      "Epoch 33/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6950 - loss: 0.5818 - val_accuracy: 0.7608 - val_loss: 0.5574\n",
      "Epoch 34/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6950 - loss: 0.5801 - val_accuracy: 0.7601 - val_loss: 0.5576\n",
      "Epoch 35/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6948 - loss: 0.5785 - val_accuracy: 0.7591 - val_loss: 0.5580\n",
      "Epoch 36/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6958 - loss: 0.5769 - val_accuracy: 0.7580 - val_loss: 0.5584\n",
      "Epoch 37/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6966 - loss: 0.5754 - val_accuracy: 0.7559 - val_loss: 0.5588\n",
      "Epoch 38/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6978 - loss: 0.5739 - val_accuracy: 0.7486 - val_loss: 0.5591\n",
      "Epoch 39/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6994 - loss: 0.5724 - val_accuracy: 0.7494 - val_loss: 0.5595\n",
      "Epoch 40/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7006 - loss: 0.5710 - val_accuracy: 0.7454 - val_loss: 0.5601\n",
      "Epoch 41/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7013 - loss: 0.5696 - val_accuracy: 0.7448 - val_loss: 0.5607\n",
      "Epoch 42/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7016 - loss: 0.5682 - val_accuracy: 0.7451 - val_loss: 0.5612\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_04\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define architecture here\n",
    "model1b = keras.Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model1b.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "model1b.add(layers.Dense(30, activation= \"relu\"))\n",
    "\n",
    "model1b.add(layers.Dense(15, activation= \"relu\"))\n",
    "\n",
    "model1b.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile model here \n",
    "\n",
    "myopt1b = Adam(learning_rate = 0.00003)\n",
    "model1b.compile(loss = \"binary_crossentropy\", optimizer = myopt1b, metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit model here\n",
    "model1b_fit = model1b.fit(x = X_train, y = y_train,\n",
    "                          epochs = 250,\n",
    "                          batch_size = 256,\n",
    "                          validation_split= 0.25,\n",
    "                          callbacks = [EarlyStopping(patience=20)])\n",
    "\n",
    "\n",
    "when_earlystop_1d = \"It stopped at epoch 42\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9a95a-7770-493e-9596-52d9ee9aeefc",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "Even though we haven't finished training our neural net, let us use the `evaluate()` function to measure the predictive performance of `model1b` on the test data. Save the result as `res_model1`. \n",
    "\n",
    "What is the accuracy of the model for validation training data and test data, respectively? What is the difference in accuracy? Save your answer in the string variable `difference_in_accuracy_1e`.\n",
    "*Hint:* the training validation accuracy can be extracted from `model1b_fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb727607-775d-44a3-a437-de1d7db42459",
   "metadata": {
    "tags": [
     "code_chunk_05"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 977us/step - accuracy: 0.7075 - loss: 0.5687\n",
      "[0.5687230229377747, 0.7075332403182983]\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_05\n",
    "res_model1 = model1b.evaluate(X_test, y_test)\n",
    "print(res_model1)\n",
    "difference_in_accuracy_1e = \"Training accuracy is 0.7016, Validation accuracy is 0.7451, Test accuracy is 0.7075.\" \\\n",
    "\"The difference in accuracy between training validation data and test data is 0.037\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef98a6a-2952-4733-a42e-45a44cdcc7f9",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "Now we use the `confusion_matrix()` function from the `metrics` module of scikit-learn to disaggregate model performance to class-specific performance. First, get class predictions on the test data using `predict()`. Save these as `prob_model1`. \n",
    "Second, use `confusion_matrix()` to get a confusion matrix and save it as `CM_model1`. Third, calculate true positive rate and false positive rate and save them as `TPR_1f` and `FPR_1f`. \n",
    "Do your results on TPR and FPR suggest that prediction accuracy is approximately equal in both categories? Write your (specific!) answer into the string variable `categorywise_accuracy_1f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d16957-1fcb-48b0-891c-9b778149090c",
   "metadata": {
    "tags": [
     "code_chunk_06"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step\n",
      "[[10560  2321]\n",
      " [ 3025  2373]]\n",
      "0.43960726194886995\n",
      "0.18018787361229718\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_06\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prob_model1 = model1b.predict(X_test) \n",
    "\n",
    "\n",
    "CM_model1   = confusion_matrix(y_test, prob_model1 > 0.5)\n",
    "\n",
    "TN = CM_model1[0,0]\n",
    "TP = CM_model1[1,1]\n",
    "FN = CM_model1[1,0]\n",
    "FP = CM_model1[0,1]\n",
    "\n",
    "\n",
    "TPR_1f = TP / (FN + TP)\n",
    "FPR_1f = FP / (FP + TN)\n",
    "\n",
    "print(CM_model1)\n",
    "print(TPR_1f)\n",
    "print(FPR_1f)\n",
    "\n",
    "categorywise_accuracy_1f = \"It is not approximately equal. he true positive rate is approximately 0.44,\" \\\n",
    "\"meaning that a majority of positive observations are missclassified. However the false positive rate is approximately 0.18 meaning that,\" \\\n",
    "\"only around a fifth of the negative observations are missclassified. So it is muchbetetr at predicting the negative category.\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6fa2-89ad-4d87-bc6e-5031b13b1f7b",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "In the lectures we have utilized explicit regularization to avoid overfitting. Here we will use $\\ell_2$ regularization to update the weights. Create the architecture of a new neural net `model2` which is identical to that of `model1b` except for $\\ell_2$ regularization with regularization factor `l2_pen` in the two hidden layers.  \n",
    "\n",
    "Then compile and fit this regularized model with the same parameters as in Task 1d. Save the trained neural net as `model2_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7505b575-e5e1-4d4d-b42b-70960a1222d9",
   "metadata": {
    "tags": [
     "code_chunk_07"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5024 - loss: 1.0794 - val_accuracy: 0.4792 - val_loss: 1.0516\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5332 - loss: 1.0429 - val_accuracy: 0.5717 - val_loss: 0.9828\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5551 - loss: 1.0161 - val_accuracy: 0.6410 - val_loss: 0.9359\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5737 - loss: 0.9948 - val_accuracy: 0.6922 - val_loss: 0.9032\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5867 - loss: 0.9764 - val_accuracy: 0.7265 - val_loss: 0.8785\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5984 - loss: 0.9599 - val_accuracy: 0.7440 - val_loss: 0.8592\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6131 - loss: 0.9450 - val_accuracy: 0.7534 - val_loss: 0.8434\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6267 - loss: 0.9312 - val_accuracy: 0.7739 - val_loss: 0.8300\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6343 - loss: 0.9183 - val_accuracy: 0.7839 - val_loss: 0.8177\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6367 - loss: 0.9062 - val_accuracy: 0.7914 - val_loss: 0.8065\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6434 - loss: 0.8947 - val_accuracy: 0.8074 - val_loss: 0.7961\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6489 - loss: 0.8838 - val_accuracy: 0.8156 - val_loss: 0.7865\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6545 - loss: 0.8736 - val_accuracy: 0.8209 - val_loss: 0.7771\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6604 - loss: 0.8639 - val_accuracy: 0.8219 - val_loss: 0.7684\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6614 - loss: 0.8547 - val_accuracy: 0.8129 - val_loss: 0.7602\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6632 - loss: 0.8460 - val_accuracy: 0.8216 - val_loss: 0.7523\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6658 - loss: 0.8376 - val_accuracy: 0.8241 - val_loss: 0.7451\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6684 - loss: 0.8296 - val_accuracy: 0.8263 - val_loss: 0.7384\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6704 - loss: 0.8219 - val_accuracy: 0.8264 - val_loss: 0.7321\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6712 - loss: 0.8145 - val_accuracy: 0.8271 - val_loss: 0.7261\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6733 - loss: 0.8075 - val_accuracy: 0.8287 - val_loss: 0.7205\n",
      "Epoch 22/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6777 - loss: 0.8008 - val_accuracy: 0.8273 - val_loss: 0.7151\n",
      "Epoch 23/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6794 - loss: 0.7943 - val_accuracy: 0.8368 - val_loss: 0.7099\n",
      "Epoch 24/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6799 - loss: 0.7881 - val_accuracy: 0.8376 - val_loss: 0.7051\n",
      "Epoch 25/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6820 - loss: 0.7821 - val_accuracy: 0.8385 - val_loss: 0.7005\n",
      "Epoch 26/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6839 - loss: 0.7764 - val_accuracy: 0.8357 - val_loss: 0.6961\n",
      "Epoch 27/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6849 - loss: 0.7709 - val_accuracy: 0.8376 - val_loss: 0.6919\n",
      "Epoch 28/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6856 - loss: 0.7657 - val_accuracy: 0.8346 - val_loss: 0.6878\n",
      "Epoch 29/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6870 - loss: 0.7606 - val_accuracy: 0.8357 - val_loss: 0.6839\n",
      "Epoch 30/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6875 - loss: 0.7557 - val_accuracy: 0.8260 - val_loss: 0.6806\n",
      "Epoch 31/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.7510 - val_accuracy: 0.8242 - val_loss: 0.6773\n",
      "Epoch 32/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6896 - loss: 0.7465 - val_accuracy: 0.8239 - val_loss: 0.6741\n",
      "Epoch 33/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6906 - loss: 0.7421 - val_accuracy: 0.8237 - val_loss: 0.6712\n",
      "Epoch 34/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6921 - loss: 0.7379 - val_accuracy: 0.8235 - val_loss: 0.6685\n",
      "Epoch 35/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6938 - loss: 0.7338 - val_accuracy: 0.8241 - val_loss: 0.6656\n",
      "Epoch 36/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6951 - loss: 0.7299 - val_accuracy: 0.8251 - val_loss: 0.6629\n",
      "Epoch 37/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6969 - loss: 0.7260 - val_accuracy: 0.8248 - val_loss: 0.6603\n",
      "Epoch 38/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6973 - loss: 0.7223 - val_accuracy: 0.8227 - val_loss: 0.6578\n",
      "Epoch 39/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6972 - loss: 0.7187 - val_accuracy: 0.8221 - val_loss: 0.6556\n",
      "Epoch 40/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6972 - loss: 0.7153 - val_accuracy: 0.8186 - val_loss: 0.6536\n",
      "Epoch 41/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6974 - loss: 0.7120 - val_accuracy: 0.8200 - val_loss: 0.6518\n",
      "Epoch 42/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6993 - loss: 0.7088 - val_accuracy: 0.8193 - val_loss: 0.6502\n",
      "Epoch 43/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7017 - loss: 0.7057 - val_accuracy: 0.8217 - val_loss: 0.6487\n",
      "Epoch 44/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7029 - loss: 0.7027 - val_accuracy: 0.8202 - val_loss: 0.6473\n",
      "Epoch 45/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7033 - loss: 0.6999 - val_accuracy: 0.8143 - val_loss: 0.6458\n",
      "Epoch 46/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7043 - loss: 0.6970 - val_accuracy: 0.8141 - val_loss: 0.6442\n",
      "Epoch 47/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7073 - loss: 0.6943 - val_accuracy: 0.8104 - val_loss: 0.6427\n",
      "Epoch 48/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7085 - loss: 0.6916 - val_accuracy: 0.8100 - val_loss: 0.6413\n",
      "Epoch 49/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7094 - loss: 0.6889 - val_accuracy: 0.8082 - val_loss: 0.6401\n",
      "Epoch 50/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7099 - loss: 0.6864 - val_accuracy: 0.8057 - val_loss: 0.6390\n",
      "Epoch 51/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7119 - loss: 0.6838 - val_accuracy: 0.8038 - val_loss: 0.6379\n",
      "Epoch 52/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7142 - loss: 0.6814 - val_accuracy: 0.8031 - val_loss: 0.6369\n",
      "Epoch 53/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7161 - loss: 0.6790 - val_accuracy: 0.8037 - val_loss: 0.6359\n",
      "Epoch 54/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7176 - loss: 0.6766 - val_accuracy: 0.8035 - val_loss: 0.6351\n",
      "Epoch 55/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7178 - loss: 0.6744 - val_accuracy: 0.8036 - val_loss: 0.6343\n",
      "Epoch 56/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7178 - loss: 0.6722 - val_accuracy: 0.8015 - val_loss: 0.6335\n",
      "Epoch 57/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7187 - loss: 0.6700 - val_accuracy: 0.8014 - val_loss: 0.6329\n",
      "Epoch 58/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7190 - loss: 0.6679 - val_accuracy: 0.8004 - val_loss: 0.6322\n",
      "Epoch 59/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7189 - loss: 0.6658 - val_accuracy: 0.8066 - val_loss: 0.6316\n",
      "Epoch 60/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.6638 - val_accuracy: 0.8044 - val_loss: 0.6309\n",
      "Epoch 61/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7194 - loss: 0.6618 - val_accuracy: 0.8044 - val_loss: 0.6303\n",
      "Epoch 62/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.6599 - val_accuracy: 0.8012 - val_loss: 0.6299\n",
      "Epoch 63/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7199 - loss: 0.6580 - val_accuracy: 0.7998 - val_loss: 0.6295\n",
      "Epoch 64/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7214 - loss: 0.6561 - val_accuracy: 0.7987 - val_loss: 0.6292\n",
      "Epoch 65/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7227 - loss: 0.6543 - val_accuracy: 0.7960 - val_loss: 0.6287\n",
      "Epoch 66/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.6525 - val_accuracy: 0.7971 - val_loss: 0.6283\n",
      "Epoch 67/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.6507 - val_accuracy: 0.7964 - val_loss: 0.6281\n",
      "Epoch 68/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7242 - loss: 0.6490 - val_accuracy: 0.7980 - val_loss: 0.6278\n",
      "Epoch 69/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7244 - loss: 0.6473 - val_accuracy: 0.7874 - val_loss: 0.6277\n",
      "Epoch 70/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7257 - loss: 0.6456 - val_accuracy: 0.7868 - val_loss: 0.6276\n",
      "Epoch 71/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7271 - loss: 0.6440 - val_accuracy: 0.7858 - val_loss: 0.6276\n",
      "Epoch 72/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.6424 - val_accuracy: 0.7868 - val_loss: 0.6277\n",
      "Epoch 73/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7298 - loss: 0.6408 - val_accuracy: 0.7839 - val_loss: 0.6279\n",
      "Epoch 74/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7309 - loss: 0.6392 - val_accuracy: 0.7809 - val_loss: 0.6281\n",
      "Epoch 75/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7322 - loss: 0.6376 - val_accuracy: 0.7807 - val_loss: 0.6284\n",
      "Epoch 76/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7330 - loss: 0.6360 - val_accuracy: 0.7757 - val_loss: 0.6286\n",
      "Epoch 77/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7348 - loss: 0.6344 - val_accuracy: 0.7755 - val_loss: 0.6286\n",
      "Epoch 78/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7358 - loss: 0.6329 - val_accuracy: 0.7746 - val_loss: 0.6287\n",
      "Epoch 79/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7363 - loss: 0.6313 - val_accuracy: 0.7767 - val_loss: 0.6290\n",
      "Epoch 80/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.6298 - val_accuracy: 0.7766 - val_loss: 0.6294\n",
      "Epoch 81/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7386 - loss: 0.6283 - val_accuracy: 0.7732 - val_loss: 0.6296\n",
      "Epoch 82/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7391 - loss: 0.6268 - val_accuracy: 0.7722 - val_loss: 0.6298\n",
      "Epoch 83/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7407 - loss: 0.6254 - val_accuracy: 0.7712 - val_loss: 0.6302\n",
      "Epoch 84/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7418 - loss: 0.6240 - val_accuracy: 0.7672 - val_loss: 0.6306\n",
      "Epoch 85/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7430 - loss: 0.6226 - val_accuracy: 0.7648 - val_loss: 0.6310\n",
      "Epoch 86/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7443 - loss: 0.6212 - val_accuracy: 0.7615 - val_loss: 0.6315\n",
      "Epoch 87/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7453 - loss: 0.6198 - val_accuracy: 0.7552 - val_loss: 0.6320\n",
      "Epoch 88/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7457 - loss: 0.6184 - val_accuracy: 0.7546 - val_loss: 0.6323\n",
      "Epoch 89/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.6170 - val_accuracy: 0.7544 - val_loss: 0.6329\n",
      "Epoch 90/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7481 - loss: 0.6157 - val_accuracy: 0.7524 - val_loss: 0.6334\n",
      "Epoch 91/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7508 - loss: 0.6143 - val_accuracy: 0.7528 - val_loss: 0.6341\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_07\n",
    "from keras.regularizers import l2\n",
    "l2_pen = 0.005\n",
    "\n",
    "# 1.\n",
    "model2 = keras.Sequential()\n",
    "\n",
    "model2.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "model2.add(layers.Dense(30, activation= \"relu\", kernel_regularizer= l2(l2_pen)))\n",
    "\n",
    "model2.add(layers.Dense(15, activation= \"relu\", kernel_regularizer= l2(l2_pen)))\n",
    "\n",
    "# 2.\n",
    "\n",
    "model2.add(layers.Dense(1, activation = \"sigmoid\", kernel_regularizer= l2(l2_pen)))\n",
    "myopt2 = Adam(learning_rate = 0.00003)\n",
    "model2.compile(loss = \"binary_crossentropy\", optimizer = myopt2, metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "model2_fit = model2.fit(x = X_train, y = y_train,\n",
    "                          epochs = 250,\n",
    "                          batch_size = 256,\n",
    "                          validation_split= 0.25,\n",
    "                          callbacks = [EarlyStopping(patience=20)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a8d24-9b08-48fe-b120-ca68106406fb",
   "metadata": {},
   "source": [
    "### Task 1h)\n",
    "In Task 1e) we compared the prediction accuracy on test and training sets. However, this is bad measure when the data is not well balanced (in terms of the observed output categories). Instead, one can use the cross entropy for the binomial distribution (minus the average log likelihood of the model). In fact, we chose this function as loss function for model training when we compiled `model1` and `model2`.\n",
    "\n",
    "To compare the test error of `model2` to that of `model1b` we don't want to use `loss` from `evaluate` since this includes the $\\ell_2$ penalty.  In the library `MLmetrics` the function `log_loss()` computes the cross entropy for the binomial distribution without penalty term. \n",
    "\n",
    "First, get predicted output probabilities on test data from `model2` and save them as `prob_model2`.\n",
    "Second, use `log_loss()` from the metrics module in scikit-learn to compute the cross-entropy loss for `model2` on the test data and save it as `logloss_model2`. \n",
    "Third, use the string variable `performance_comparison_1h` to describe how the accuracy on test data differs between `model1b` and `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a39ea-f599-4f1d-9ca0-4e54aa598cb9",
   "metadata": {
    "tags": [
     "code_chunk_08"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618us/step\n",
      "0.533637640646477\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_08\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. \n",
    "prob_model2    = model2.predict(X_test)\n",
    "\n",
    "# 2.\n",
    "logloss_model2 = log_loss(y_test, prob_model2)\n",
    "print(logloss_model2)\n",
    "\n",
    "# 3.\n",
    "performance_comparison_1h = \"In model 1b the loss on test data was 0.5687. In model2 the loss is 0.535 so the accuracy has gone down.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8bf1-f86e-4b0c-9cc6-bdd9f1d964c4",
   "metadata": {},
   "source": [
    "## Part 2: Tuning neural nets with caret\n",
    "\n",
    "Keras provides functions that allow the use of scikit-learn for model tuning. Using this functionality requires relatively little effort and in this part we are going to practice the individual steps of model tuning with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac151a-c3c3-44e8-8adb-a8a3cedd6d95",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "First, we need to define the architecture of the neural net that we tune and to compile the model. This needs to be done inside a function. The arguments of this function are the tuning parameters whose candidate values we want to feed into the function one by one.\n",
    "\n",
    "In this task, we work with the architecture defined for `model2` in Task 1g. The only parameter that we want to tune is the regularization parameter for $\\ell_2$ penalization inside the hidden units.\n",
    "\n",
    "Now create a function `modelbuild_2a` which has one argument `l2_pen`. In this function, specify the architecture of a Keras model `model`, identical to that of `model2` and with $\\ell_2$-penalty set to `l2_pen`. Compile the model with the same settings as in Task 1g and return it as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecfec657-d581-4b0c-b751-114dc1496386",
   "metadata": {
    "tags": [
     "code_chunk_09"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_09\n",
    "def modelbuild_2a(l2_pen):\n",
    "    \n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    model.add(layers.Dense(30, activation= \"relu\", kernel_regularizer= l2(l2_pen)))\n",
    "\n",
    "    model.add(layers.Dense(15, activation= \"relu\", kernel_regularizer= l2(l2_pen)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation = \"sigmoid\", kernel_regularizer= l2(l2_pen)))\n",
    "    myopt = Adam(learning_rate = 0.00003)\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = myopt, metrics = [\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078933-0ccc-4f9e-abb6-e26520bd8760",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "In order to make the function `modelbuild_2a` compatible with `sklearn`, we need to call it inside the `KerasClassifier()` function from the `wrappers` module of `scikit-learn`. As additional arguments, provide all arguments that you used when fitting `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "027797af-2ee2-4566-8c67-ef98b0f40f8f",
   "metadata": {
    "tags": [
     "code_chunk_10"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_10\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "model2_sklearn_spec = KerasClassifier(\n",
    "                          model =modelbuild_2a,\n",
    "                          l2_pen=l2_pen,\n",
    "                          epochs = 250,\n",
    "                          batch_size = 256,\n",
    "                          validation_split=0.25,\n",
    "                          callbacks=[EarlyStopping(patience=20)]\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cce66-a557-4597-80dc-bb1ff84b4f1e",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "Next, define a parameter grid `tune_grid_2c`. This must be a dictionary object. Ensure that the only object within `tune_grid_2c` is called `l2_pen`. Its values should be zero as well as $10^{r}$ for a grid of eleven $r$-values from $-4$ and $-1$ at equal distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "63925c0d-6abd-49e6-bfad-5adc320ce894",
   "metadata": {
    "tags": [
     "code_chunk_11"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_11\n",
    "tune_grid_2c = {\"l2_pen\": [0] + [10**x for x in np.linspace(-4,-1,11)]}\n",
    "\n",
    "#tune_grid_2c = {\"l2_pen\": [0, 10**-3]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978345-6081-478b-9549-771d300a1992",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "Now, we can tune our model. That's computationally quite costly, so we will use merely a fraction of the available training data. The inputs and outputs for this task are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e12d402-63b4-420b-a4b0-8c6f2830fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_ , y_train_small, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847933-232b-499f-847a-04255215d8cd",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "1. Use `Kfold()` from the model selection module in scikit-learn to define a random partition of the training data into five folds. Use $5$ as your random seed. Save this partition as `cv_splits_2d`.\n",
    "2. Call `GridSearchCV()` and use the wrapper function from Task 2b, the parameter grid from Task 2c as well as `cv_splits_2d` as arguments.\n",
    "3. Apply the `fit()`-method to `GridSearchCV` and use `X_train_small` and `y_train_small` as inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8746fa6-0cd3-469c-afb2-7278ee02ae69",
   "metadata": {
    "tags": [
     "code_chunk_12"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4544 - loss: 0.7823 - val_accuracy: 0.4726 - val_loss: 0.7728\n",
      "Epoch 2/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4709 - loss: 0.7630 - val_accuracy: 0.4860 - val_loss: 0.7547\n",
      "Epoch 3/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4912 - loss: 0.7466 - val_accuracy: 0.5125 - val_loss: 0.7392\n",
      "Epoch 4/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5184 - loss: 0.7324 - val_accuracy: 0.5286 - val_loss: 0.7258\n",
      "Epoch 5/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5407 - loss: 0.7202 - val_accuracy: 0.5475 - val_loss: 0.7141\n",
      "Epoch 6/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5604 - loss: 0.7095 - val_accuracy: 0.5658 - val_loss: 0.7040\n",
      "Epoch 7/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5822 - loss: 0.7004 - val_accuracy: 0.5847 - val_loss: 0.6953\n",
      "Epoch 8/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5983 - loss: 0.6925 - val_accuracy: 0.6047 - val_loss: 0.6878\n",
      "Epoch 9/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6146 - loss: 0.6856 - val_accuracy: 0.6229 - val_loss: 0.6814\n",
      "Epoch 10/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6315 - loss: 0.6797 - val_accuracy: 0.6399 - val_loss: 0.6757\n",
      "Epoch 11/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6525 - loss: 0.6744 - val_accuracy: 0.6543 - val_loss: 0.6706\n",
      "Epoch 12/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6629 - loss: 0.6698 - val_accuracy: 0.6586 - val_loss: 0.6662\n",
      "Epoch 13/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6651 - loss: 0.6657 - val_accuracy: 0.6645 - val_loss: 0.6622\n",
      "Epoch 14/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6712 - loss: 0.6621 - val_accuracy: 0.6744 - val_loss: 0.6587\n",
      "Epoch 15/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6782 - loss: 0.6588 - val_accuracy: 0.6813 - val_loss: 0.6556\n",
      "Epoch 16/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6831 - loss: 0.6558 - val_accuracy: 0.6842 - val_loss: 0.6527\n",
      "Epoch 17/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6866 - loss: 0.6531 - val_accuracy: 0.6871 - val_loss: 0.6501\n",
      "Epoch 18/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6896 - loss: 0.6506 - val_accuracy: 0.6898 - val_loss: 0.6477\n",
      "Epoch 19/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6910 - loss: 0.6483 - val_accuracy: 0.6915 - val_loss: 0.6455\n",
      "Epoch 20/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6945 - loss: 0.6461 - val_accuracy: 0.6992 - val_loss: 0.6434\n",
      "Epoch 21/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6978 - loss: 0.6440 - val_accuracy: 0.7014 - val_loss: 0.6415\n",
      "Epoch 22/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6993 - loss: 0.6421 - val_accuracy: 0.7048 - val_loss: 0.6396\n",
      "Epoch 23/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7013 - loss: 0.6401 - val_accuracy: 0.7058 - val_loss: 0.6378\n",
      "Epoch 24/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7015 - loss: 0.6383 - val_accuracy: 0.7063 - val_loss: 0.6361\n",
      "Epoch 25/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7019 - loss: 0.6366 - val_accuracy: 0.7068 - val_loss: 0.6344\n",
      "Epoch 26/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7021 - loss: 0.6349 - val_accuracy: 0.7085 - val_loss: 0.6328\n",
      "Epoch 27/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7037 - loss: 0.6332 - val_accuracy: 0.7090 - val_loss: 0.6312\n",
      "Epoch 28/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 0.6316 - val_accuracy: 0.7092 - val_loss: 0.6297\n",
      "Epoch 29/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7045 - loss: 0.6301 - val_accuracy: 0.7085 - val_loss: 0.6283\n",
      "Epoch 30/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7046 - loss: 0.6286 - val_accuracy: 0.7085 - val_loss: 0.6269\n",
      "Epoch 31/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7048 - loss: 0.6272 - val_accuracy: 0.7087 - val_loss: 0.6256\n",
      "Epoch 32/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7046 - loss: 0.6258 - val_accuracy: 0.7107 - val_loss: 0.6243\n",
      "Epoch 33/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7055 - loss: 0.6244 - val_accuracy: 0.7097 - val_loss: 0.6231\n",
      "Epoch 34/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7060 - loss: 0.6231 - val_accuracy: 0.7109 - val_loss: 0.6219\n",
      "Epoch 35/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7059 - loss: 0.6218 - val_accuracy: 0.7097 - val_loss: 0.6207\n",
      "Epoch 36/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7060 - loss: 0.6205 - val_accuracy: 0.7107 - val_loss: 0.6196\n",
      "Epoch 37/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7067 - loss: 0.6193 - val_accuracy: 0.7109 - val_loss: 0.6184\n",
      "Epoch 38/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7067 - loss: 0.6181 - val_accuracy: 0.7112 - val_loss: 0.6173\n",
      "Epoch 39/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7069 - loss: 0.6169 - val_accuracy: 0.7104 - val_loss: 0.6163\n",
      "Epoch 40/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7088 - loss: 0.6158 - val_accuracy: 0.7146 - val_loss: 0.6152\n",
      "Epoch 41/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7107 - loss: 0.6146 - val_accuracy: 0.7158 - val_loss: 0.6142\n",
      "Epoch 42/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7117 - loss: 0.6135 - val_accuracy: 0.7163 - val_loss: 0.6132\n",
      "Epoch 43/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.6125 - val_accuracy: 0.7165 - val_loss: 0.6122\n",
      "Epoch 44/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7122 - loss: 0.6114 - val_accuracy: 0.7158 - val_loss: 0.6112\n",
      "Epoch 45/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7123 - loss: 0.6104 - val_accuracy: 0.7158 - val_loss: 0.6103\n",
      "Epoch 46/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7124 - loss: 0.6094 - val_accuracy: 0.7158 - val_loss: 0.6094\n",
      "Epoch 47/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7125 - loss: 0.6084 - val_accuracy: 0.7148 - val_loss: 0.6085\n",
      "Epoch 48/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7120 - loss: 0.6074 - val_accuracy: 0.7150 - val_loss: 0.6076\n",
      "Epoch 49/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7122 - loss: 0.6065 - val_accuracy: 0.7160 - val_loss: 0.6067\n",
      "Epoch 50/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7131 - loss: 0.6055 - val_accuracy: 0.7153 - val_loss: 0.6059\n",
      "Epoch 51/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7122 - loss: 0.6046 - val_accuracy: 0.7160 - val_loss: 0.6050\n",
      "Epoch 52/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7127 - loss: 0.6037 - val_accuracy: 0.7168 - val_loss: 0.6042\n",
      "Epoch 53/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7128 - loss: 0.6028 - val_accuracy: 0.7165 - val_loss: 0.6034\n",
      "Epoch 54/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7127 - loss: 0.6019 - val_accuracy: 0.7160 - val_loss: 0.6026\n",
      "Epoch 55/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7119 - loss: 0.6010 - val_accuracy: 0.7165 - val_loss: 0.6018\n",
      "Epoch 56/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7121 - loss: 0.6002 - val_accuracy: 0.7163 - val_loss: 0.6010\n",
      "Epoch 57/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7119 - loss: 0.5993 - val_accuracy: 0.7155 - val_loss: 0.6003\n",
      "Epoch 58/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7119 - loss: 0.5985 - val_accuracy: 0.7155 - val_loss: 0.5995\n",
      "Epoch 59/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7120 - loss: 0.5977 - val_accuracy: 0.7163 - val_loss: 0.5988\n",
      "Epoch 60/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7128 - loss: 0.5969 - val_accuracy: 0.7163 - val_loss: 0.5980\n",
      "Epoch 61/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7139 - loss: 0.5961 - val_accuracy: 0.7182 - val_loss: 0.5973\n",
      "Epoch 62/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7144 - loss: 0.5953 - val_accuracy: 0.7177 - val_loss: 0.5966\n",
      "Epoch 63/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7146 - loss: 0.5945 - val_accuracy: 0.7175 - val_loss: 0.5959\n",
      "Epoch 64/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7146 - loss: 0.5937 - val_accuracy: 0.7172 - val_loss: 0.5952\n",
      "Epoch 65/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7145 - loss: 0.5929 - val_accuracy: 0.7175 - val_loss: 0.5945\n",
      "Epoch 66/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7146 - loss: 0.5922 - val_accuracy: 0.7172 - val_loss: 0.5939\n",
      "Epoch 67/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7143 - loss: 0.5914 - val_accuracy: 0.7180 - val_loss: 0.5932\n",
      "Epoch 68/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7148 - loss: 0.5907 - val_accuracy: 0.7189 - val_loss: 0.5925\n",
      "Epoch 69/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7154 - loss: 0.5900 - val_accuracy: 0.7189 - val_loss: 0.5919\n",
      "Epoch 70/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7157 - loss: 0.5893 - val_accuracy: 0.7189 - val_loss: 0.5912\n",
      "Epoch 71/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7157 - loss: 0.5885 - val_accuracy: 0.7197 - val_loss: 0.5906\n",
      "Epoch 72/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7169 - loss: 0.5878 - val_accuracy: 0.7206 - val_loss: 0.5899\n",
      "Epoch 73/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7178 - loss: 0.5871 - val_accuracy: 0.7209 - val_loss: 0.5893\n",
      "Epoch 74/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7179 - loss: 0.5864 - val_accuracy: 0.7209 - val_loss: 0.5887\n",
      "Epoch 75/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7178 - loss: 0.5857 - val_accuracy: 0.7204 - val_loss: 0.5880\n",
      "Epoch 76/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7179 - loss: 0.5850 - val_accuracy: 0.7204 - val_loss: 0.5874\n",
      "Epoch 77/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.5844 - val_accuracy: 0.7194 - val_loss: 0.5868\n",
      "Epoch 78/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7174 - loss: 0.5837 - val_accuracy: 0.7187 - val_loss: 0.5862\n",
      "Epoch 79/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7174 - loss: 0.5830 - val_accuracy: 0.7187 - val_loss: 0.5857\n",
      "Epoch 80/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7182 - loss: 0.5824 - val_accuracy: 0.7211 - val_loss: 0.5851\n",
      "Epoch 81/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7194 - loss: 0.5817 - val_accuracy: 0.7219 - val_loss: 0.5845\n",
      "Epoch 82/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7199 - loss: 0.5811 - val_accuracy: 0.7219 - val_loss: 0.5840\n",
      "Epoch 83/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7207 - loss: 0.5805 - val_accuracy: 0.7233 - val_loss: 0.5834\n",
      "Epoch 84/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7212 - loss: 0.5798 - val_accuracy: 0.7238 - val_loss: 0.5829\n",
      "Epoch 85/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7214 - loss: 0.5792 - val_accuracy: 0.7238 - val_loss: 0.5824\n",
      "Epoch 86/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7212 - loss: 0.5786 - val_accuracy: 0.7243 - val_loss: 0.5818\n",
      "Epoch 87/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7214 - loss: 0.5780 - val_accuracy: 0.7243 - val_loss: 0.5813\n",
      "Epoch 88/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.5774 - val_accuracy: 0.7262 - val_loss: 0.5808\n",
      "Epoch 89/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7234 - loss: 0.5768 - val_accuracy: 0.7260 - val_loss: 0.5803\n",
      "Epoch 90/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7244 - loss: 0.5762 - val_accuracy: 0.7260 - val_loss: 0.5798\n",
      "Epoch 91/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7247 - loss: 0.5756 - val_accuracy: 0.7255 - val_loss: 0.5793\n",
      "Epoch 92/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7247 - loss: 0.5751 - val_accuracy: 0.7260 - val_loss: 0.5788\n",
      "Epoch 93/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7255 - loss: 0.5745 - val_accuracy: 0.7260 - val_loss: 0.5783\n",
      "Epoch 94/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7257 - loss: 0.5739 - val_accuracy: 0.7260 - val_loss: 0.5778\n",
      "Epoch 95/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7259 - loss: 0.5734 - val_accuracy: 0.7257 - val_loss: 0.5773\n",
      "Epoch 96/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7263 - loss: 0.5728 - val_accuracy: 0.7260 - val_loss: 0.5769\n",
      "Epoch 97/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7266 - loss: 0.5723 - val_accuracy: 0.7262 - val_loss: 0.5764\n",
      "Epoch 98/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7274 - loss: 0.5717 - val_accuracy: 0.7262 - val_loss: 0.5759\n",
      "Epoch 99/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7273 - loss: 0.5712 - val_accuracy: 0.7265 - val_loss: 0.5755\n",
      "Epoch 100/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7280 - loss: 0.5706 - val_accuracy: 0.7265 - val_loss: 0.5750\n",
      "Epoch 101/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7281 - loss: 0.5701 - val_accuracy: 0.7265 - val_loss: 0.5746\n",
      "Epoch 102/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7282 - loss: 0.5696 - val_accuracy: 0.7267 - val_loss: 0.5741\n",
      "Epoch 103/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7287 - loss: 0.5691 - val_accuracy: 0.7279 - val_loss: 0.5737\n",
      "Epoch 104/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7298 - loss: 0.5686 - val_accuracy: 0.7282 - val_loss: 0.5733\n",
      "Epoch 105/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7302 - loss: 0.5681 - val_accuracy: 0.7282 - val_loss: 0.5729\n",
      "Epoch 106/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7306 - loss: 0.5675 - val_accuracy: 0.7284 - val_loss: 0.5724\n",
      "Epoch 107/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7317 - loss: 0.5670 - val_accuracy: 0.7289 - val_loss: 0.5720\n",
      "Epoch 108/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7326 - loss: 0.5665 - val_accuracy: 0.7294 - val_loss: 0.5716\n",
      "Epoch 109/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7337 - loss: 0.5661 - val_accuracy: 0.7330 - val_loss: 0.5712\n",
      "Epoch 110/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7353 - loss: 0.5656 - val_accuracy: 0.7326 - val_loss: 0.5708\n",
      "Epoch 111/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7359 - loss: 0.5651 - val_accuracy: 0.7333 - val_loss: 0.5704\n",
      "Epoch 112/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7365 - loss: 0.5646 - val_accuracy: 0.7328 - val_loss: 0.5700\n",
      "Epoch 113/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7363 - loss: 0.5641 - val_accuracy: 0.7335 - val_loss: 0.5696\n",
      "Epoch 114/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7369 - loss: 0.5636 - val_accuracy: 0.7338 - val_loss: 0.5692\n",
      "Epoch 115/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7371 - loss: 0.5631 - val_accuracy: 0.7340 - val_loss: 0.5689\n",
      "Epoch 116/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7371 - loss: 0.5627 - val_accuracy: 0.7343 - val_loss: 0.5685\n",
      "Epoch 117/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7371 - loss: 0.5622 - val_accuracy: 0.7343 - val_loss: 0.5681\n",
      "Epoch 118/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7371 - loss: 0.5617 - val_accuracy: 0.7343 - val_loss: 0.5677\n",
      "Epoch 119/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7375 - loss: 0.5612 - val_accuracy: 0.7350 - val_loss: 0.5673\n",
      "Epoch 120/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7390 - loss: 0.5608 - val_accuracy: 0.7350 - val_loss: 0.5669\n",
      "Epoch 121/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7389 - loss: 0.5603 - val_accuracy: 0.7350 - val_loss: 0.5665\n",
      "Epoch 122/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7391 - loss: 0.5599 - val_accuracy: 0.7345 - val_loss: 0.5662\n",
      "Epoch 123/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7391 - loss: 0.5594 - val_accuracy: 0.7345 - val_loss: 0.5658\n",
      "Epoch 124/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7392 - loss: 0.5589 - val_accuracy: 0.7345 - val_loss: 0.5655\n",
      "Epoch 125/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7392 - loss: 0.5585 - val_accuracy: 0.7350 - val_loss: 0.5651\n",
      "Epoch 126/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7392 - loss: 0.5580 - val_accuracy: 0.7355 - val_loss: 0.5647\n",
      "Epoch 127/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7397 - loss: 0.5576 - val_accuracy: 0.7352 - val_loss: 0.5644\n",
      "Epoch 128/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.5571 - val_accuracy: 0.7360 - val_loss: 0.5640\n",
      "Epoch 129/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7407 - loss: 0.5567 - val_accuracy: 0.7367 - val_loss: 0.5637\n",
      "Epoch 130/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7411 - loss: 0.5562 - val_accuracy: 0.7369 - val_loss: 0.5633\n",
      "Epoch 131/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7409 - loss: 0.5558 - val_accuracy: 0.7372 - val_loss: 0.5630\n",
      "Epoch 132/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7413 - loss: 0.5554 - val_accuracy: 0.7372 - val_loss: 0.5626\n",
      "Epoch 133/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7413 - loss: 0.5549 - val_accuracy: 0.7372 - val_loss: 0.5623\n",
      "Epoch 134/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7413 - loss: 0.5545 - val_accuracy: 0.7374 - val_loss: 0.5620\n",
      "Epoch 135/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.5541 - val_accuracy: 0.7379 - val_loss: 0.5616\n",
      "Epoch 136/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.5536 - val_accuracy: 0.7374 - val_loss: 0.5613\n",
      "Epoch 137/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 0.5532 - val_accuracy: 0.7374 - val_loss: 0.5610\n",
      "Epoch 138/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.5528 - val_accuracy: 0.7374 - val_loss: 0.5606\n",
      "Epoch 139/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7427 - loss: 0.5524 - val_accuracy: 0.7374 - val_loss: 0.5603\n",
      "Epoch 140/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7430 - loss: 0.5520 - val_accuracy: 0.7374 - val_loss: 0.5600\n",
      "Epoch 141/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7430 - loss: 0.5516 - val_accuracy: 0.7374 - val_loss: 0.5597\n",
      "Epoch 142/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7430 - loss: 0.5511 - val_accuracy: 0.7374 - val_loss: 0.5593\n",
      "Epoch 143/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7430 - loss: 0.5507 - val_accuracy: 0.7386 - val_loss: 0.5590\n",
      "Epoch 144/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.5503 - val_accuracy: 0.7384 - val_loss: 0.5587\n",
      "Epoch 145/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7436 - loss: 0.5499 - val_accuracy: 0.7389 - val_loss: 0.5584\n",
      "Epoch 146/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7440 - loss: 0.5495 - val_accuracy: 0.7391 - val_loss: 0.5581\n",
      "Epoch 147/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.5491 - val_accuracy: 0.7391 - val_loss: 0.5578\n",
      "Epoch 148/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.5487 - val_accuracy: 0.7394 - val_loss: 0.5575\n",
      "Epoch 149/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.5483 - val_accuracy: 0.7394 - val_loss: 0.5572\n",
      "Epoch 150/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.5479 - val_accuracy: 0.7391 - val_loss: 0.5568\n",
      "Epoch 151/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7448 - loss: 0.5475 - val_accuracy: 0.7391 - val_loss: 0.5565\n",
      "Epoch 152/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7448 - loss: 0.5471 - val_accuracy: 0.7396 - val_loss: 0.5562\n",
      "Epoch 153/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7452 - loss: 0.5467 - val_accuracy: 0.7403 - val_loss: 0.5559\n",
      "Epoch 154/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7457 - loss: 0.5463 - val_accuracy: 0.7406 - val_loss: 0.5556\n",
      "Epoch 155/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7463 - loss: 0.5459 - val_accuracy: 0.7406 - val_loss: 0.5553\n",
      "Epoch 156/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7472 - loss: 0.5455 - val_accuracy: 0.7420 - val_loss: 0.5550\n",
      "Epoch 157/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7480 - loss: 0.5451 - val_accuracy: 0.7418 - val_loss: 0.5547\n",
      "Epoch 158/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7479 - loss: 0.5447 - val_accuracy: 0.7416 - val_loss: 0.5544\n",
      "Epoch 159/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7495 - loss: 0.5443 - val_accuracy: 0.7445 - val_loss: 0.5541\n",
      "Epoch 160/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7504 - loss: 0.5439 - val_accuracy: 0.7447 - val_loss: 0.5538\n",
      "Epoch 161/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7510 - loss: 0.5435 - val_accuracy: 0.7450 - val_loss: 0.5536\n",
      "Epoch 162/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7512 - loss: 0.5432 - val_accuracy: 0.7450 - val_loss: 0.5533\n",
      "Epoch 163/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7516 - loss: 0.5428 - val_accuracy: 0.7452 - val_loss: 0.5530\n",
      "Epoch 164/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7520 - loss: 0.5424 - val_accuracy: 0.7452 - val_loss: 0.5527\n",
      "Epoch 165/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7522 - loss: 0.5420 - val_accuracy: 0.7452 - val_loss: 0.5524\n",
      "Epoch 166/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7522 - loss: 0.5416 - val_accuracy: 0.7457 - val_loss: 0.5521\n",
      "Epoch 167/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.5412 - val_accuracy: 0.7454 - val_loss: 0.5518\n",
      "Epoch 168/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7530 - loss: 0.5408 - val_accuracy: 0.7464 - val_loss: 0.5516\n",
      "Epoch 169/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7534 - loss: 0.5404 - val_accuracy: 0.7481 - val_loss: 0.5513\n",
      "Epoch 170/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7539 - loss: 0.5400 - val_accuracy: 0.7476 - val_loss: 0.5510\n",
      "Epoch 171/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7542 - loss: 0.5396 - val_accuracy: 0.7481 - val_loss: 0.5507\n",
      "Epoch 172/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7546 - loss: 0.5392 - val_accuracy: 0.7484 - val_loss: 0.5504\n",
      "Epoch 173/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7547 - loss: 0.5389 - val_accuracy: 0.7484 - val_loss: 0.5502\n",
      "Epoch 174/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7552 - loss: 0.5385 - val_accuracy: 0.7491 - val_loss: 0.5499\n",
      "Epoch 175/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7558 - loss: 0.5381 - val_accuracy: 0.7491 - val_loss: 0.5496\n",
      "Epoch 176/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7559 - loss: 0.5377 - val_accuracy: 0.7496 - val_loss: 0.5493\n",
      "Epoch 177/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7564 - loss: 0.5373 - val_accuracy: 0.7496 - val_loss: 0.5491\n",
      "Epoch 178/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7567 - loss: 0.5369 - val_accuracy: 0.7496 - val_loss: 0.5488\n",
      "Epoch 179/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7566 - loss: 0.5365 - val_accuracy: 0.7496 - val_loss: 0.5485\n",
      "Epoch 180/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7562 - loss: 0.5361 - val_accuracy: 0.7491 - val_loss: 0.5483\n",
      "Epoch 181/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7564 - loss: 0.5358 - val_accuracy: 0.7486 - val_loss: 0.5480\n",
      "Epoch 182/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7565 - loss: 0.5354 - val_accuracy: 0.7486 - val_loss: 0.5478\n",
      "Epoch 183/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.5350 - val_accuracy: 0.7503 - val_loss: 0.5475\n",
      "Epoch 184/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7582 - loss: 0.5346 - val_accuracy: 0.7503 - val_loss: 0.5472\n",
      "Epoch 185/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7585 - loss: 0.5342 - val_accuracy: 0.7513 - val_loss: 0.5470\n",
      "Epoch 186/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7591 - loss: 0.5338 - val_accuracy: 0.7522 - val_loss: 0.5467\n",
      "Epoch 187/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7593 - loss: 0.5334 - val_accuracy: 0.7522 - val_loss: 0.5464\n",
      "Epoch 188/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7593 - loss: 0.5331 - val_accuracy: 0.7527 - val_loss: 0.5462\n",
      "Epoch 189/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7601 - loss: 0.5327 - val_accuracy: 0.7532 - val_loss: 0.5459\n",
      "Epoch 190/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7604 - loss: 0.5323 - val_accuracy: 0.7532 - val_loss: 0.5457\n",
      "Epoch 191/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7607 - loss: 0.5319 - val_accuracy: 0.7532 - val_loss: 0.5454\n",
      "Epoch 192/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.5315 - val_accuracy: 0.7547 - val_loss: 0.5451\n",
      "Epoch 193/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.5311 - val_accuracy: 0.7547 - val_loss: 0.5449\n",
      "Epoch 194/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7614 - loss: 0.5308 - val_accuracy: 0.7552 - val_loss: 0.5446\n",
      "Epoch 195/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7614 - loss: 0.5304 - val_accuracy: 0.7552 - val_loss: 0.5444\n",
      "Epoch 196/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7616 - loss: 0.5300 - val_accuracy: 0.7554 - val_loss: 0.5441\n",
      "Epoch 197/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.5296 - val_accuracy: 0.7554 - val_loss: 0.5439\n",
      "Epoch 198/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7632 - loss: 0.5293 - val_accuracy: 0.7586 - val_loss: 0.5436\n",
      "Epoch 199/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7644 - loss: 0.5289 - val_accuracy: 0.7581 - val_loss: 0.5434\n",
      "Epoch 200/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7646 - loss: 0.5285 - val_accuracy: 0.7581 - val_loss: 0.5431\n",
      "Epoch 201/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7649 - loss: 0.5281 - val_accuracy: 0.7583 - val_loss: 0.5429\n",
      "Epoch 202/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7646 - loss: 0.5278 - val_accuracy: 0.7576 - val_loss: 0.5426\n",
      "Epoch 203/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.5274 - val_accuracy: 0.7576 - val_loss: 0.5423\n",
      "Epoch 204/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7648 - loss: 0.5270 - val_accuracy: 0.7576 - val_loss: 0.5421\n",
      "Epoch 205/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7649 - loss: 0.5267 - val_accuracy: 0.7576 - val_loss: 0.5418\n",
      "Epoch 206/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7653 - loss: 0.5263 - val_accuracy: 0.7588 - val_loss: 0.5416\n",
      "Epoch 207/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7657 - loss: 0.5259 - val_accuracy: 0.7588 - val_loss: 0.5414\n",
      "Epoch 208/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7654 - loss: 0.5256 - val_accuracy: 0.7591 - val_loss: 0.5411\n",
      "Epoch 209/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7662 - loss: 0.5252 - val_accuracy: 0.7598 - val_loss: 0.5409\n",
      "Epoch 210/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.5249 - val_accuracy: 0.7595 - val_loss: 0.5406\n",
      "Epoch 211/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.5245 - val_accuracy: 0.7600 - val_loss: 0.5404\n",
      "Epoch 212/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.5241 - val_accuracy: 0.7605 - val_loss: 0.5401\n",
      "Epoch 213/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.5238 - val_accuracy: 0.7605 - val_loss: 0.5399\n",
      "Epoch 214/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.5234 - val_accuracy: 0.7605 - val_loss: 0.5396\n",
      "Epoch 215/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7667 - loss: 0.5231 - val_accuracy: 0.7605 - val_loss: 0.5394\n",
      "Epoch 216/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.5227 - val_accuracy: 0.7598 - val_loss: 0.5392\n",
      "Epoch 217/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.5223 - val_accuracy: 0.7598 - val_loss: 0.5389\n",
      "Epoch 218/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7666 - loss: 0.5220 - val_accuracy: 0.7598 - val_loss: 0.5387\n",
      "Epoch 219/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7666 - loss: 0.5216 - val_accuracy: 0.7598 - val_loss: 0.5384\n",
      "Epoch 220/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7670 - loss: 0.5213 - val_accuracy: 0.7605 - val_loss: 0.5382\n",
      "Epoch 221/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7674 - loss: 0.5209 - val_accuracy: 0.7608 - val_loss: 0.5380\n",
      "Epoch 222/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.5206 - val_accuracy: 0.7603 - val_loss: 0.5377\n",
      "Epoch 223/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7683 - loss: 0.5202 - val_accuracy: 0.7629 - val_loss: 0.5375\n",
      "Epoch 224/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.5199 - val_accuracy: 0.7634 - val_loss: 0.5373\n",
      "Epoch 225/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7692 - loss: 0.5195 - val_accuracy: 0.7639 - val_loss: 0.5370\n",
      "Epoch 226/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.5192 - val_accuracy: 0.7639 - val_loss: 0.5368\n",
      "Epoch 227/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.5188 - val_accuracy: 0.7637 - val_loss: 0.5366\n",
      "Epoch 228/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.5185 - val_accuracy: 0.7637 - val_loss: 0.5364\n",
      "Epoch 229/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.5181 - val_accuracy: 0.7637 - val_loss: 0.5361\n",
      "Epoch 230/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.5178 - val_accuracy: 0.7637 - val_loss: 0.5359\n",
      "Epoch 231/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7698 - loss: 0.5174 - val_accuracy: 0.7637 - val_loss: 0.5357\n",
      "Epoch 232/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7706 - loss: 0.5171 - val_accuracy: 0.7642 - val_loss: 0.5355\n",
      "Epoch 233/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7712 - loss: 0.5167 - val_accuracy: 0.7646 - val_loss: 0.5352\n",
      "Epoch 234/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.5164 - val_accuracy: 0.7646 - val_loss: 0.5350\n",
      "Epoch 235/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7725 - loss: 0.5161 - val_accuracy: 0.7646 - val_loss: 0.5348\n",
      "Epoch 236/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7726 - loss: 0.5157 - val_accuracy: 0.7646 - val_loss: 0.5346\n",
      "Epoch 237/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7726 - loss: 0.5154 - val_accuracy: 0.7651 - val_loss: 0.5344\n",
      "Epoch 238/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7730 - loss: 0.5150 - val_accuracy: 0.7649 - val_loss: 0.5342\n",
      "Epoch 239/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7734 - loss: 0.5147 - val_accuracy: 0.7651 - val_loss: 0.5340\n",
      "Epoch 240/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.5144 - val_accuracy: 0.7651 - val_loss: 0.5337\n",
      "Epoch 241/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.5140 - val_accuracy: 0.7651 - val_loss: 0.5335\n",
      "Epoch 242/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7737 - loss: 0.5137 - val_accuracy: 0.7651 - val_loss: 0.5333\n",
      "Epoch 243/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7737 - loss: 0.5133 - val_accuracy: 0.7649 - val_loss: 0.5331\n",
      "Epoch 244/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7737 - loss: 0.5130 - val_accuracy: 0.7651 - val_loss: 0.5329\n",
      "Epoch 245/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.5127 - val_accuracy: 0.7651 - val_loss: 0.5327\n",
      "Epoch 246/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.5123 - val_accuracy: 0.7654 - val_loss: 0.5325\n",
      "Epoch 247/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7744 - loss: 0.5120 - val_accuracy: 0.7654 - val_loss: 0.5322\n",
      "Epoch 248/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7746 - loss: 0.5116 - val_accuracy: 0.7654 - val_loss: 0.5320\n",
      "Epoch 249/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7749 - loss: 0.5113 - val_accuracy: 0.7654 - val_loss: 0.5318\n",
      "Epoch 250/250\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7751 - loss: 0.5110 - val_accuracy: 0.7654 - val_loss: 0.5316\n",
      "Cross validation finished in 803.5843682289124 seconds\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_12\n",
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1.\n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "\n",
    "# 2.\n",
    "#NN_tune_2d = GridSearchCV(estimator=model2_sklearn_spec, param_grid= tune_grid_2c, cv= cv_splits_2d,n_jobs=4) we used more cores to speed up training\n",
    "NN_tune_2d = GridSearchCV(estimator=model2_sklearn_spec, param_grid= tune_grid_2c, cv= cv_splits_2d) #Slow as it only uses one core\n",
    "\n",
    "# 3.\n",
    "NN_tune_2d.fit(\n",
    "    X=X_train_small, \n",
    "    y=y_train_small\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Cross validation finished in {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16a15-f9d0-4d38-896a-627e4d5dde75",
   "metadata": {},
   "source": [
    "### Task 2e)\n",
    "By default, GridSearchCV saves the trained model with the best tuning parameter values as as object `best_estimator_` inside `NN_tune_2d`. However, in our case this model is a KerasClassifier object. We cannot use such an object for making predictions on test data. \n",
    "\n",
    "Still, the KerasClassifier object contains the trained model in the typical Keras format as object `model`. Extract this object-inside-the-object-inside-the-object and save it as `model3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed6ec904-ce98-4907-b3fa-cc54b1a43cd1",
   "metadata": {
    "tags": [
     "code_chunk_13"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_13\n",
    "model3 = NN_tune_2d.best_estimator_.model_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6448b-ac63-4129-9236-6dcb23bcf1dd",
   "metadata": {},
   "source": [
    "## Part 3: Saving, loading and retraining neural nets \n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "Training and tuning neural nets can take a lot of time. Therefore, it is possible to save entire fitted models to disk and to import them at a later point in time. Apply the `save()`-method to your most recent `model3` in order to save it as *DABN13_asst6_saved_model3* in your working directory.\n",
    "\n",
    "Ensure that the model is saved in TensorFlow SavedModel format.\n",
    "\n",
    "Additionally, use the file explorer in your operating system to look how exactly the model was saved on your hard drive. Describe this shortly in the string_variable `saved_model_3a`.\n",
    "\n",
    "*Note:* Functions for saving and loading models are very nicely described in the [keras documentation](https://keras.io/api/saving/model_saving_and_loading/#save-method).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e1779-a59c-4153-9c89-39d3be986fbc",
   "metadata": {
    "tags": [
     "code_chunk_14"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DABN13_asst6_saved_model3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: DABN13_asst6_saved_model3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'DABN13_asst6_saved_model3'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 123), dtype=tf.float32, name='keras_tensor_129')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2044674332944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2044674327952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2044674326992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2044674341968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2044674327184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2044674340048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_14\n",
    "model3.export(\"DABN13_asst6_saved_model3\") #We need this to answer 3a) but we cannot use it for later tasks\n",
    "model3.save(\"DABN13_asst6_saved_model3.keras\") #Keras 3 can no longer refit models saved in tensorflow format so we need to save in keras format also\n",
    "saved_model_3a = \"It is saved as a folder containing a saved_model_pb, as well as a folder containing the variables.\" \\\n",
    "\"There is also an assets folder that is empty as well as a fingerprint fi.pb file. \" \\\n",
    "\"This is a legacy format so we also save a model in the .keras format that results in one single file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424edf9-37e1-4b9e-9c91-4836c6367709",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Now, use the `load_model` function in the `models` module of Keras to load your saved model into your python session again. Save this model as `model4`.\n",
    "\n",
    "*Note:* The possibility to load a previously saved model from your hard disk is useful for more than just your own models. It even allows you to load pretrained models for specific purposes from the TensorFlow Hub or from Hugging Face. These models could then directly be used for prediction or fine-tuned on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f89a6c-9730-4dbd-950f-2d5b5f2f0da6",
   "metadata": {
    "tags": [
     "code_chunk_15"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_15\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model4 = load_model(\"DABN13_asst6_saved_model3.keras\") #We are using this as refitting a SavedModel is no longer supported in keras 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385071-18a5-4ca3-906e-895fdeb9c692",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "When we tuned our most recent neural net, we did this on a relatively small fraction of the training data to reduce the computational cost. This was also the data used to train the best model that we extracted from `NN_tune_2d`.\n",
    "Now that we have chosen an optimal tuning parameter, it makes sense to retrain the `model4` on the entire training data `X_train` and `y_train`. Do this by applying the `fit()` method to `model4`. As previously, training should be done for 250 epochs, unless early stopping with a patience of 20 epochs kicks in. Minibatches of $2^8$ data points should be used. Given the large amount of data, hold only 10% of the data aside for monitoring validation loss.\n",
    "\n",
    "Once you retrained your model, obtain predicted class probabilities and save them as `prob_model4`. Then, get the log loss `logloss_model4` on the test data.\n",
    "\n",
    "Finally, to what extend did model tuning and retraining change test set accuracy relative to that of `model2`? Comment on this in the string variable `performance_comparison_3c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f945e31-63c3-48ef-a5e2-54e7b36cdddc",
   "metadata": {
    "tags": [
     "code_chunk_16"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7056 - loss: 0.5951 - val_accuracy: 0.9314 - val_loss: 0.3771\n",
      "Epoch 2/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7162 - loss: 0.5868 - val_accuracy: 0.9113 - val_loss: 0.3998\n",
      "Epoch 3/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7193 - loss: 0.5830 - val_accuracy: 0.9062 - val_loss: 0.4109\n",
      "Epoch 4/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7218 - loss: 0.5800 - val_accuracy: 0.9020 - val_loss: 0.4183\n",
      "Epoch 5/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7229 - loss: 0.5774 - val_accuracy: 0.8991 - val_loss: 0.4243\n",
      "Epoch 6/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7258 - loss: 0.5750 - val_accuracy: 0.8963 - val_loss: 0.4296\n",
      "Epoch 7/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7282 - loss: 0.5729 - val_accuracy: 0.8928 - val_loss: 0.4341\n",
      "Epoch 8/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7305 - loss: 0.5709 - val_accuracy: 0.8867 - val_loss: 0.4382\n",
      "Epoch 9/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7320 - loss: 0.5691 - val_accuracy: 0.8838 - val_loss: 0.4421\n",
      "Epoch 10/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7335 - loss: 0.5674 - val_accuracy: 0.8786 - val_loss: 0.4456\n",
      "Epoch 11/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7357 - loss: 0.5657 - val_accuracy: 0.8765 - val_loss: 0.4487\n",
      "Epoch 12/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7386 - loss: 0.5642 - val_accuracy: 0.8743 - val_loss: 0.4516\n",
      "Epoch 13/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7394 - loss: 0.5626 - val_accuracy: 0.8711 - val_loss: 0.4544\n",
      "Epoch 14/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7412 - loss: 0.5612 - val_accuracy: 0.8672 - val_loss: 0.4572\n",
      "Epoch 15/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7424 - loss: 0.5597 - val_accuracy: 0.8669 - val_loss: 0.4598\n",
      "Epoch 16/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7433 - loss: 0.5584 - val_accuracy: 0.8633 - val_loss: 0.4623\n",
      "Epoch 17/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.5570 - val_accuracy: 0.8576 - val_loss: 0.4647\n",
      "Epoch 18/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7456 - loss: 0.5557 - val_accuracy: 0.8567 - val_loss: 0.4669\n",
      "Epoch 19/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7464 - loss: 0.5544 - val_accuracy: 0.8547 - val_loss: 0.4690\n",
      "Epoch 20/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.5532 - val_accuracy: 0.8481 - val_loss: 0.4710\n",
      "Epoch 21/250\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7474 - loss: 0.5519 - val_accuracy: 0.8464 - val_loss: 0.4728\n",
      "\u001b[1m572/572\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step\n",
      "0.5158994077361431\n"
     ]
    }
   ],
   "source": [
    "# CODE_CHUNK code_chunk_16\n",
    "# 1.\n",
    "model4_fit = model4.fit(x = X_train, y = y_train,\n",
    "                          epochs = 250,\n",
    "                          batch_size = 256,\n",
    "                          validation_split= 0.25,\n",
    "                          callbacks = [EarlyStopping(patience=20)])\n",
    "# 2.\n",
    "prob_model4    = model4.predict(X_test)\n",
    "logloss_model4 = log_loss(y_test, prob_model4)\n",
    "print(logloss_model4)\n",
    "\n",
    "# 3.\n",
    "performance_comparison_3c = \"The test accuracy of model 2 is 0.7440. The test accuracy of model 4 is 0.7641 so it has improved slightly.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766e3e-925d-434b-a546-35b8a77fa177",
   "metadata": {},
   "source": [
    "## Part 4: Manual predictions from a trained neural net\n",
    "\n",
    "In this part we will build predictions *manually* by extracting weights from the trained  `model2` and by constructing the transformations in the layers of the neural net ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3b6b3-6107-41d3-8bd4-09cc1c393791",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "\n",
    "Start this task by creating your own ReLU activation function. Save it as `ReLU`. Then, write your own sigmoid function for the output layer transformation. Save it as  `sigmoid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "755df496-d94c-48b7-8a0d-d406e94d6e0a",
   "metadata": {
    "tags": [
     "code_chunk_17"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_17\n",
    "# ReLU activation function\n",
    "def ReLU(x):\n",
    "    y = np.maximum(0,x)\n",
    "    return y\n",
    "\n",
    "\n",
    "# Sigmoid activation function\n",
    "def Sigmoid(x):\n",
    "    y = (1 + np.exp(-x))**-1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8157-ef24-4994-a4bd-34827db28d8e",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "In the slides for lecture 8 we discussed how units in the different layers of a neural net look like. Now we are going to use the equations both hidden units and output unit to construct output predictions for the $n_{test}$ data points in `X_test`. Please do the following:\n",
    "\n",
    "1. Apply the `get_weights()` method on `model2` to obtain a list object which stores the weights and biases of the learned model. Save it as `weight_and_bias_4b`.\n",
    "2. Extract the objects inside `weight_and_bias_4b` into the objects for $\\mathbf{b}_1,\\mathbf{W}_1, \\mathbf{b}_2, \\mathbf{W}_2,\\mathbf{b}_3, \\mathbf{W}_3$ defined in the code chunk below.\n",
    "3. Construct the linear term of the first layer hidden units and save these linear terms as a $30 \\times n_{train}$ vector `Z_1`.\n",
    "4. Construct the $15 \\times n_{train}$ vector `Z_2` of linear terms for the second layer hidden units. Then, obtain the linear term of the output unit and save it as `Z_3`.\n",
    "5. Put `Z_3` into the output layer activation function in order to get predictions for the probability of a Bud Light purchase. Save this as $n_{train} \\times 1$ vector `pred_own_2b`. \n",
    "\n",
    "*Hint*: You can use `dim()` to check the dimension of matrices and `length()` to check that of vectors. Additionally, to ensure that you got the correct result for `prob_model2_own_4b` you can compare it with the output of `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "50523588-05be-4174-b294-392f96cd8f86",
   "metadata": {
    "tags": [
     "code_chunk_18"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_18\n",
    "# 1.\n",
    "weight_and_bias_4b = model2.get_weights() \n",
    "\n",
    "# 2.\n",
    "WW_1 = weight_and_bias_4b[0]\n",
    "bb_1 = weight_and_bias_4b[1]\n",
    "WW_2 = weight_and_bias_4b[2]\n",
    "bb_2 = weight_and_bias_4b[3]\n",
    "WW_3 = weight_and_bias_4b[4]\n",
    "bb_3 = weight_and_bias_4b[5]\n",
    "\n",
    "# 3.\n",
    "Z_1 = (bb_1 + X_test@WW_1).T\n",
    "\n",
    "# 4.\n",
    "Z_2 = (bb_2 + ReLU(Z_1).T@WW_2).T\n",
    "Z_3 = (bb_3 + ReLU(Z_2).T@WW_3).T\n",
    "\n",
    "# Apply sigmoid activation to Z_3\n",
    "prob_model2_own_4b = Sigmoid(Z_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STAN51",
   "language": "python",
   "name": "stan51"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
