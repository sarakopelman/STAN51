{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b1cb77-5344-418c-94b3-7bacb800c746",
   "metadata": {},
   "source": [
    "# DABN13 - Assignment 6\n",
    "\n",
    "## Preamble: Data\n",
    "In this lab we are using a dataset on beer purchases. Our goal is to predict if light beer purchased in the US is BUD light. To achieve this goal, we will use the information provided by the following socioeconomic characteristics:\n",
    "* market           - where the beer is bought\n",
    "* buyertype        - who is the buyer () \n",
    "* income           - ranges of income\n",
    "* childrenUnder6   - does the buyer have children under 6 years old\n",
    "* children6to17    - does the buyer have children between 6 and 17\n",
    "* age              - bracketed age groups\n",
    "* employment       - fully employed, partially employed, no employment.\n",
    "* degree           - level of occupation\n",
    "* occuptation      - which sector you are employed in\n",
    "* ethnic           - white, asian, hispanic, black or other\n",
    "* microwave        - own a microwave\n",
    "* dishwasher       - own a dishwasher\n",
    "* tvcable          - what type cable tv subscription you have\n",
    "* singlefamilyhome - are you in a single family home\n",
    "* npeople          - number of people you live with 1,2,3,4, +5\n",
    "\n",
    "First, we load the dataset and create an output variable that indicates purchases of Bud Light."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cb558-0655-4ff6-80d8-51cc89e3c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# os.chdir(\"?\") Change working directory if needed \n",
    "\n",
    "lb    = pd.read_csv(\"LightBeer2.csv\")\n",
    "y     = np.zeros(shape=lb.shape[0])\n",
    "y[lb['beer_brand'] == \"BUD LIGHT\"]     = 1\n",
    "demog = lb.iloc[:,9:]\n",
    "demog = pd.get_dummies(demog, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174fd7-d779-4207-a30e-224c89bdcbc9",
   "metadata": {},
   "source": [
    "We also split the data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c287b-7069-426c-b3c1-15857ff123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(demog, y, train_size=0.75, shuffle=False)\n",
    "\n",
    "stdz_X = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = stdz_X.transform(X_train)\n",
    "X_test  = stdz_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983d48-57e5-4342-9935-16676fd3a106",
   "metadata": {},
   "source": [
    "## Part 1: Specifying and training neural networks\n",
    "We will now start building a neural network to predict the purchase of Bud Light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7cab-178e-4fee-b942-8ab68a140e23",
   "metadata": {},
   "source": [
    "### Task 1a) \n",
    "We start with specifying the architecture of our very first and very small neural net `model1`.\n",
    "Add three layers to `model1`, two hidden layers with $30$ and $15$ hidden units, respectively, and an output layer.\n",
    "For the two hidden layers you should use the ReLU activation function. Additionally, choose a suitable activation for the output layer, given that we have a classification problem. See [the Keras documentation](https://keras.io/api/layers/activations/) for activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9388d-cbd6-4d10-90ff-2c3d974a8238",
   "metadata": {
    "tags": [
     "code_chunk_01"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_01\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize a first model\n",
    "model1 = ??\n",
    "\n",
    "# Add layers to the model\n",
    "??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bcd214-22bf-42fa-a973-2c27f0643431",
   "metadata": {},
   "source": [
    "### Task 1b) \n",
    "\n",
    "Next, we compile our model specification. From [https://keras.io/api/losses/probabilistic_losses/](losses) select a suitable loss function for our classification problem. As optimization algorithm use *Adam* with learning rate $0.00003$. Lastly, use `accuracy` as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88ddd5-3e85-4238-96a3-81b914f4fc9f",
   "metadata": {
    "tags": [
     "code_chunk_02"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_02\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import ??\n",
    "\n",
    "# Compile the model\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2904463-9a53-4ea3-b175-adfc9ff0f599",
   "metadata": {},
   "source": [
    "### Task 1c)\n",
    "Now train the model using $250$ epochs, a batch_size of $2^8$, and use $25\\%$ of the data for validation.\n",
    "Use the string variable `loss_valloss_difference_1c` to describe and explain the observed difference between validation loss and training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f00f52b-92c7-4100-bfe0-05cdd6f0fe63",
   "metadata": {
    "tags": [
     "code_chunk_03"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_03\n",
    "??\n",
    "\n",
    "loss_valloss_difference_1c = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74189128-f787-4b8c-8c96-6b782065dad7",
   "metadata": {},
   "source": [
    "### Task 1d)\n",
    "In Lecture 9 we used early stopping to avoid overfitting. Apply this here, with `patience` argument set to 20. Create a new model, `model1b`, which otherwise should have a setup identical to `model1`. In which epoch did the model training procedure stop? Figure this out by counting the number of elements in `model1b_fit.history['loss']` and write your answer into the string variable `when_earlystop_1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13f2cb-b29b-4288-9b8d-e69f331faff4",
   "metadata": {
    "tags": [
     "code_chunk_04"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_04\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define architecture here\n",
    "??\n",
    "\n",
    "# Compile model here \n",
    "??\n",
    "\n",
    "# Fit model here\n",
    "model1b_fit = ??\n",
    "\n",
    "\n",
    "when_earlystop_1d = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9a95a-7770-493e-9596-52d9ee9aeefc",
   "metadata": {},
   "source": [
    "### Task 1e)\n",
    "Even though we haven't finished training our neural net, let us use the `evaluate()` function to measure the predictive performance of `model1b` on the test data. Save the result as `res_model1`. \n",
    "\n",
    "What is the accuracy of the model for validation training data and test data, respectively? What is the difference in accuracy? Save your answer in the string variable `difference_in_accuracy_1e`.\n",
    "*Hint:* the training validation accuracy can be extracted from `model1b_fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb727607-775d-44a3-a437-de1d7db42459",
   "metadata": {
    "tags": [
     "code_chunk_05"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_05\n",
    "res_model1 = ??\n",
    "print(res_model1)\n",
    "difference_in_accuracy_1e = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef98a6a-2952-4733-a42e-45a44cdcc7f9",
   "metadata": {},
   "source": [
    "### Task 1f)\n",
    "Now we use the `confusion_matrix()` function from the `metrics` module of scikit-learn to disaggregate model performance to class-specific performance. First, get class predictions on the test data using `predict()`. Save these as `prob_model1`. \n",
    "Second, use `confusion_matrix()` to get a confusion matrix and save it as `CM_model1`. Third, calculate true positive rate and false positive rate and save them as `TPR_1f` and `FPR_1f`. \n",
    "Do your results on TPR and FPR suggest that prediction accuracy is approximately equal in both categories? Write your (specific!) answer into the string variable `categorywise_accuracy_1f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d16957-1fcb-48b0-891c-9b778149090c",
   "metadata": {
    "tags": [
     "code_chunk_06"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_06\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prob_model1 = ??\n",
    "CM_model1   = ??\n",
    "\n",
    "TN = ??\n",
    "TP = ??\n",
    "FN = ??\n",
    "FP = ??\n",
    "\n",
    "\n",
    "TPR_1f = ??\n",
    "FPR_1f = ??\n",
    "\n",
    "print(CM_model1)\n",
    "print(TPR_1f)\n",
    "print(FPR_1f)\n",
    "\n",
    "categorywise_accuracy_1f = \"??\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229b6fa2-89ad-4d87-bc6e-5031b13b1f7b",
   "metadata": {},
   "source": [
    "### Task 1g)\n",
    "\n",
    "In the lectures we have utilized explicit regularization to avoid overfitting. Here we will use $\\ell_2$ regularization to update the weights. Create the architecture of a new neural net `model2` which is identical to that of `model1b` except for $\\ell_2$ regularization with regularization factor `l2_pen` in the two hidden layers.  \n",
    "\n",
    "Then compile and fit this regularized model with the same parameters as in Task 1d. Save the trained neural net as `model2_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505b575-e5e1-4d4d-b42b-70960a1222d9",
   "metadata": {
    "tags": [
     "code_chunk_07"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_07\n",
    "from tensorflow.keras.regularizers import l2\n",
    "l2_pen = 0.005\n",
    "\n",
    "# 1.\n",
    "??\n",
    "\n",
    "# 2.\n",
    "??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a8d24-9b08-48fe-b120-ca68106406fb",
   "metadata": {},
   "source": [
    "### Task 1h)\n",
    "In Task 1e) we compared the prediction accuracy on test and training sets. However, this is bad measure when the data is not well balanced (in terms of the observed output categories). Instead, one can use the cross entropy for the binomial distribution (minus the average log likelihood of the model). In fact, we chose this function as loss function for model training when we compiled `model1` and `model2`.\n",
    "\n",
    "To compare the test error of `model2` to that of `model1b` we don't want to use `loss` from `evaluate` since this includes the $\\ell_2$ penalty.  In the library `MLmetrics` the function `log_loss()` computes the cross entropy for the binomial distribution without penalty term. \n",
    "\n",
    "First, get predicted output probabilities on test data from `model2` and save them as `prob_model2`.\n",
    "Second, use `log_loss()` from the metrics module in scikit-learn to compute the cross-entropy loss for `model2` on the test data and save it as `logloss_model2`. \n",
    "Third, use the string variable `performance_comparison_1h` to describe how the accuracy on test data differs between `model1b` and `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a39ea-f599-4f1d-9ca0-4e54aa598cb9",
   "metadata": {
    "tags": [
     "code_chunk_08"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_08\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 1. \n",
    "prob_model2    = ??\n",
    "\n",
    "# 2.\n",
    "logloss_model2 = ??\n",
    "print(logloss_model2)\n",
    "\n",
    "# 3.\n",
    "performance_comparison_1h = \"??\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8bf1-f86e-4b0c-9cc6-bdd9f1d964c4",
   "metadata": {},
   "source": [
    "## Part 2: Tuning neural nets with caret\n",
    "\n",
    "Keras provides functions that allow the use of scikit-learn for model tuning. Using this functionality requires relatively little effort and in this part we are going to practice the individual steps of model tuning with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac151a-c3c3-44e8-8adb-a8a3cedd6d95",
   "metadata": {},
   "source": [
    "### Task 2a)\n",
    "First, we need to define the architecture of the neural net that we tune and to compile the model. This needs to be done inside a function. The arguments of this function are the tuning parameters whose candidate values we want to feed into the function one by one.\n",
    "\n",
    "In this task, we work with the architecture defined for `model2` in Task 1g. The only parameter that we want to tune is the regularization parameter for $\\ell_2$ penalization inside the hidden units.\n",
    "\n",
    "Now create a function `modelbuild_2a` which has one argument `l2_pen`. In this function, specify the architecture of a Keras model `model`, identical to that of `model2` and with $\\ell_2$-penalty set to `l2_pen`. Compile the model with the same settings as in Task 1g and return it as function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec657-d581-4b0c-b751-114dc1496386",
   "metadata": {
    "tags": [
     "code_chunk_09"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_09\n",
    "??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64078933-0ccc-4f9e-abb6-e26520bd8760",
   "metadata": {},
   "source": [
    "### Task 2b)\n",
    "In order to make the function `modelbuild_2a` compatible with `sklearn`, we need to call it inside the `KerasClassifier()` function from the `wrappers` module of `scikit-learn`. As additional arguments, provide all arguments that you used when fitting `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027797af-2ee2-4566-8c67-ef98b0f40f8f",
   "metadata": {
    "tags": [
     "code_chunk_10"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_10\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "model2_sklearn_spec = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cce66-a557-4597-80dc-bb1ff84b4f1e",
   "metadata": {},
   "source": [
    "### Task 2c)\n",
    "Next, define a parameter grid `tune_grid_2c`. This must be a dictionary object. Ensure that the only object within `tune_grid_2c` is called `l2_pen`. Its values should be zero as well as $10^{r}$ for a grid of eleven $r$-values from $-4$ and $-1$ at equal distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63925c0d-6abd-49e6-bfad-5adc320ce894",
   "metadata": {
    "tags": [
     "code_chunk_11"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_11\n",
    "tune_grid_2c = {\n",
    "??\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978345-6081-478b-9549-771d300a1992",
   "metadata": {},
   "source": [
    "### Task 2d)\n",
    "Now, we can tune our model. That's computationally quite costly, so we will use merely a fraction of the available training data. The inputs and outputs for this task are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12d402-63b4-420b-a4b0-8c6f2830fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small,_ , y_train_small, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69847933-232b-499f-847a-04255215d8cd",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "\n",
    "1. Use `Kfold()` from the model selection module in scikit-learn to define a random partition of the training data into five folds. Use $5$ as your random seed. Save this partition as `cv_splits_2d`.\n",
    "2. Call `GridSearchCV()` and use the wrapper function from Task 2b, the parameter grid from Task 2c as well as `cv_splits_2d` as arguments.\n",
    "3. Apply the `fit()`-method to `GridSearchCV` and use `X_train_small` and `y_train_small` as inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8746fa6-0cd3-469c-afb2-7278ee02ae69",
   "metadata": {
    "tags": [
     "code_chunk_12"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_12\n",
    "from sklearn.model_selection import (KFold, GridSearchCV)\n",
    "cv_splits_2d = KFold(n_splits=5, shuffle=True, random_state=5)\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 1.\n",
    "cv_splits_2d = ??\n",
    "\n",
    "# 2.\n",
    "NN_tune_2d = ??\n",
    "\n",
    "# 3.\n",
    "??\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Cross validation finished in {elapsed_time} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c16a15-f9d0-4d38-896a-627e4d5dde75",
   "metadata": {},
   "source": [
    "### Task 2e)\n",
    "By default, GridSearchCV saves the trained model with the best tuning parameter values as as object `best_estimator_` inside `NN_tune_2d`. However, in our case this model is a KerasClassifier object. We cannot use such an object for making predictions on test data. \n",
    "\n",
    "Still, the KerasClassifier object contains the trained model in the typical Keras format as object `model`. Extract this object-inside-the-object-inside-the-object and save it as `model3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ec904-ce98-4907-b3fa-cc54b1a43cd1",
   "metadata": {
    "tags": [
     "code_chunk_13"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_13\n",
    "model3 = ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6448b-ac63-4129-9236-6dcb23bcf1dd",
   "metadata": {},
   "source": [
    "## Part 3: Saving, loading and retraining neural nets \n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "Training and tuning neural nets can take a lot of time. Therefore, it is possible to save entire fitted models to disk and to import them at a later point in time. Apply the `save()`-method to your most recent `model3` in order to save it as *DABN13_asst6_saved_model3* in your working directory.\n",
    "\n",
    "Ensure that the model is saved in TensorFlow SavedModel format.\n",
    "\n",
    "Additionally, use the file explorer in your operating system to look how exactly the model was saved on your hard drive. Describe this shortly in the string_variable `saved_model_3a`.\n",
    "\n",
    "*Note:* Functions for saving and loading models are very nicely described in the [keras documentation](https://keras.io/api/saving/model_saving_and_loading/#save-method).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e1779-a59c-4153-9c89-39d3be986fbc",
   "metadata": {
    "tags": [
     "code_chunk_14"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_14\n",
    "??\n",
    "\n",
    "saved_model_3a = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424edf9-37e1-4b9e-9c91-4836c6367709",
   "metadata": {},
   "source": [
    "### Task 3b)\n",
    "\n",
    "Now, use the `load_model` function in the `models` module of Keras to load your saved model into your python session again. Save this model as `model4`.\n",
    "\n",
    "*Note:* The possibility to load a previously saved model from your hard disk is useful for more than just your own models. It even allows you to load pretrained models for specific purposes from the TensorFlow Hub or from Hugging Face. These models could then directly be used for prediction or fine-tuned on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f89a6c-9730-4dbd-950f-2d5b5f2f0da6",
   "metadata": {
    "tags": [
     "code_chunk_15"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_15\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model4 = ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83385071-18a5-4ca3-906e-895fdeb9c692",
   "metadata": {},
   "source": [
    "### Task 3c)\n",
    "\n",
    "When we tuned our most recent neural net, we did this on a relatively small fraction of the training data to reduce the computational cost. This was also the data used to train the best model that we extracted from `NN_tune_2d`.\n",
    "Now that we have chosen an optimal tuning parameter, it makes sense to retrain the `model4` on the entire training data `X_train` and `y_train`. Do this by applying the `fit()` method to `model4`. As previously, training should be done for 250 epochs, unless early stopping with a patience of 20 epochs kicks in. Minibatches of $2^8$ data points should be used. Given the large amount of data, hold only 10% of the data aside for monitoring validation loss.\n",
    "\n",
    "Once you retrained your model, obtain predicted class probabilities and save them as `prob_model4`. Then, get the log loss `logloss_model4` on the test data.\n",
    "\n",
    "Finally, to what extend did model tuning and retraining change test set accuracy relative to that of `model2`? Comment on this in the string variable `performance_comparison_3c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f945e31-63c3-48ef-a5e2-54e7b36cdddc",
   "metadata": {
    "tags": [
     "code_chunk_16"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_16\n",
    "# 1.\n",
    "??\n",
    "\n",
    "# 2.\n",
    "prob_model4    = ??\n",
    "logloss_model4 = ??\n",
    "print(logloss_model4)\n",
    "\n",
    "# 3.\n",
    "performance_comparison_3c = \"??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766e3e-925d-434b-a546-35b8a77fa177",
   "metadata": {},
   "source": [
    "## Part 4: Manual predictions from a trained neural net\n",
    "\n",
    "In this part we will build predictions *manually* by extracting weights from the trained  `model2` and by constructing the transformations in the layers of the neural net ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3b6b3-6107-41d3-8bd4-09cc1c393791",
   "metadata": {},
   "source": [
    "### Task 4a)\n",
    "\n",
    "Start this task by creating your own ReLU activation function. Save it as `ReLU`. Then, write your own sigmoid function for the output layer transformation. Save it as  `sigmoid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755df496-d94c-48b7-8a0d-d406e94d6e0a",
   "metadata": {
    "tags": [
     "code_chunk_17"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_17\n",
    "# ReLU activation function\n",
    "??\n",
    "\n",
    "# Sigmoid activation function\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d8157-ef24-4994-a4bd-34827db28d8e",
   "metadata": {},
   "source": [
    "### Task 4b)\n",
    "In the slides for lecture 8 we discussed how units in the different layers of a neural net look like. Now we are going to use the equations both hidden units and output unit to construct output predictions for the $n_{test}$ data points in `X_test`. Please do the following:\n",
    "\n",
    "1. Apply the `get_weights()` method on `model2` to obtain a list object which stores the weights and biases of the learned model. Save it as `weight_and_bias_4b`.\n",
    "2. Extract the objects inside `weight_and_bias_4b` into the objects for $\\mathbf{b}_1,\\mathbf{W}_1, \\mathbf{b}_2, \\mathbf{W}_2,\\mathbf{b}_3, \\mathbf{W}_3$ defined in the code chunk below.\n",
    "3. Construct the linear term of the first layer hidden units and save these linear terms as a $30 \\times n_{train}$ vector `Z_1`.\n",
    "4. Construct the $15 \\times n_{train}$ vector `Z_2` of linear terms for the second layer hidden units. Then, obtain the linear term of the output unit and save it as `Z_3`.\n",
    "5. Put `Z_3` into the output layer activation function in order to get predictions for the probability of a Bud Light purchase. Save this as $n_{train} \\times 1$ vector `pred_own_2b`. \n",
    "\n",
    "*Hint*: You can use `dim()` to check the dimension of matrices and `length()` to check that of vectors. Additionally, to ensure that you got the correct result for `prob_model2_own_4b` you can compare it with the output of `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50523588-05be-4174-b294-392f96cd8f86",
   "metadata": {
    "tags": [
     "code_chunk_18"
    ]
   },
   "outputs": [],
   "source": [
    "# CODE_CHUNK code_chunk_18\n",
    "# 1.\n",
    "weight_and_bias_4b = ??\n",
    "\n",
    "# 2.\n",
    "WW_1 = ??\n",
    "bb_1 = ??\n",
    "WW_2 = ??\n",
    "bb_2 = ??\n",
    "WW_3 = ??\n",
    "bb_3 = ??\n",
    "\n",
    "# 3.\n",
    "Z_1 = ??\n",
    "\n",
    "# 4.\n",
    "Z_2 = ??\n",
    "Z_3 = ??\n",
    "\n",
    "# Apply sigmoid activation to Z_3\n",
    "prob_model2_own_4b = ??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
